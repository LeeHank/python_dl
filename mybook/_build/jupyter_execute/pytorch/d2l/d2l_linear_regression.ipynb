{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Linear regression (d2l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 修改自 d2l 的 3.1 ~ 3.3 (線性回歸從0開始實現、線性回歸的簡潔實現）([link](https://zh.d2l.ai/chapter_linear-networks/linear-regression-scratch.html))\n",
    "* 這一章，我們要:\n",
    "  * 先 靠自己刻出 linear regression (from scratch)\n",
    "  * 再用 pytorch 內建 NN 來做做看. \n",
    "  * 比較 自己寫的 和 內建 的差異"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "## 生 data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* linear model 定義為： $\\mathbf{y}= \\mathbf{X} \\mathbf{w} + b + \\mathbf\\epsilon.$\n",
    "* 其中， y 是 n 維向量，w 是 p 維向量 (feature維度)，X 是 nxp 矩陣，b 是 scalar\n",
    "* 先寫一個 function 來產生模擬資料："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_data(w, b, num_examples):  #@save\n",
    "    p = len(w) # feature 個數\n",
    "    X = torch.normal(0, 1, (num_examples, p)) # 生出 shape = (n, p) 的 matrix\n",
    "    y = torch.matmul(X, w) + b\n",
    "    y += torch.normal(0, 0.01, y.shape)\n",
    "    return X, y.reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 現在來產出模擬資料：  \n",
    "  * n = 1000, p = 2\n",
    "  * w = [2, -3.4], b = 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8164, -0.9453],\n",
       "        [-1.2195,  0.1272],\n",
       "        [ 0.5601,  1.0297],\n",
       "        ...,\n",
       "        [ 0.6262, -1.1752],\n",
       "        [ 0.6752, -1.0719],\n",
       "        [ 0.6124,  0.5205]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.8100],\n",
       "        [1.3357],\n",
       "        [1.8175],\n",
       "        [5.5317],\n",
       "        [4.9031]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 畫一下 y 對 x1, y 對 x2 的 scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "filenames": {
       "image/png": "/Volumes/GoogleDrive/我的雲端硬碟/0. codepool_python/python_dl/mybook/_build/jupyter_execute/pytorch/d2l/d2l_linear_regression_11_0.png"
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(features[:, 0].detach().numpy(), labels.detach().numpy());\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "filenames": {
       "image/png": "/Volumes/GoogleDrive/我的雲端硬碟/0. codepool_python/python_dl/mybook/_build/jupyter_execute/pytorch/d2l/d2l_linear_regression_12_0.png"
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(features[:, 1].detach().numpy(), labels.detach().numpy());\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "### data iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 我們要來做出一個 iterator，在讀資料時，每次都可以吐 batch size 的資料出來  \n",
    "* 這邊複習一下 `range(0, 10, 3)` 的意思，是 0~10，每 3 個取一個，所以結果會是： [0, 3, 6, 9]，最後一個分不完就不取了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "origin_pos": 13,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def my_data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples)) # [0,1,...,n]\n",
    "    random.shuffle(indices) # [4,1,25,0,...]\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        batch_indices = torch.tensor(\n",
    "            indices[i: min(i + batch_size, num_examples)])\n",
    "        yield features[batch_indices], labels[batch_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 驗證一下，如果 batch size = 10，看一下第一個 batch 的長相"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0324, -0.3804],\n",
      "        [-0.2023,  0.1426],\n",
      "        [ 0.0364,  1.5655],\n",
      "        [-1.3299, -0.7282],\n",
      "        [-0.1377, -0.3729],\n",
      "        [ 1.6993,  0.4536],\n",
      "        [-0.4657, -1.1417],\n",
      "        [-1.0266, -0.1638],\n",
      "        [-0.3879,  1.4113],\n",
      "        [ 0.2974,  0.6664]]) \n",
      " tensor([[ 5.5652],\n",
      "        [ 3.3013],\n",
      "        [-1.0554],\n",
      "        [ 4.0234],\n",
      "        [ 5.1900],\n",
      "        [ 6.0698],\n",
      "        [ 7.1442],\n",
      "        [ 2.7238],\n",
      "        [-1.3799],\n",
      "        [ 2.5420]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "custom_data_iter = my_data_iter(10, features, labels)\n",
    "\n",
    "for X, y in custom_data_iter:\n",
    "    print(X, '\\n', y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定義模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, w, b):  #@save\n",
    "    \"\"\" linear regression \"\"\"\n",
    "    return torch.matmul(X, w) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 23
   },
   "source": [
    "### loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "origin_pos": 24,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class MyMSE:\n",
    "    def __init__(self, reduction = \"mean\"):\n",
    "        self.reduction = reduction\n",
    "    def __call__(self, y_hat, y):\n",
    "        loss = (y_hat - y.reshape(y_hat.shape)) ** 2\n",
    "        if self.reduction == \"mean\":\n",
    "            cost = loss.mean()\n",
    "        if self.reduction == \"sum\":\n",
    "            cost = loss.sum()\n",
    "        if self.reduction == \"none\":\n",
    "            cost = loss\n",
    "        return cost\n",
    "\n",
    "# instance\n",
    "loss = MyMSE(reduction = \"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 25
   },
   "source": [
    "### optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "origin_pos": 27,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def optimizer(params, lr = 0.03):  #@save\n",
    "    \"\"\" sgd \"\"\"\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 29
   },
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 走一個 batch 來看看發生什麼事"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "lr = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.6985,  0.0291],\n",
      "        [-0.0222,  0.0066],\n",
      "        [ 1.1131, -1.0533],\n",
      "        [ 0.7180,  0.5768],\n",
      "        [ 0.0799, -0.5924],\n",
      "        [ 0.1805,  0.9742],\n",
      "        [ 0.3105,  1.2170],\n",
      "        [ 0.3885,  0.6850],\n",
      "        [ 0.8224, -0.2691],\n",
      "        [-0.2953, -1.0634]])\n",
      "tensor([[0.6720],\n",
      "        [4.1316],\n",
      "        [9.9998],\n",
      "        [3.6875],\n",
      "        [6.3652],\n",
      "        [1.2642],\n",
      "        [0.6865],\n",
      "        [2.6544],\n",
      "        [6.7504],\n",
      "        [7.2257]])\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "custom_data_iter = my_data_iter(10, features, labels)\n",
    "X, y = next(iter(custom_data_iter))\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化參數\n",
    "w = torch.normal(0, 0.01, size=(2,1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 看一下初始化的參數，和真值的差距"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimate w: tensor([[-0.0079],\n",
      "        [-0.0013]], requires_grad=True)\n",
      "true w: tensor([ 2.0000, -3.4000])\n",
      "mse of parameter estimation: 15.583215713500977\n"
     ]
    }
   ],
   "source": [
    "print(f\"estimate w: {w}\")\n",
    "print(f\"true w: {true_w}\")\n",
    "print(f\"mse of parameter estimation: {((w.reshape(-1)-true_w)**2).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimate b: tensor([0.], requires_grad=True)\n",
      "true b: 4.2\n",
      "mse of parameter estimation: 17.64\n"
     ]
    }
   ],
   "source": [
    "print(f\"estimate b: {b}\")\n",
    "print(f\"true b: {true_b}\")\n",
    "print(f\"mse of parameter estimation: {((b.item()-true_b)**2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 可以看到，差距蠻遠的。\n",
    "* 每個參數，都有他目前對應的 gradient，可以這樣看："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 可以看到，目前 w, b 都沒有 gradient  \n",
    "* 接著，來做第一次 forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25.1611, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y_hat = model(X, w, b)\n",
    "batch_cost = loss(y_hat, y)\n",
    "print(batch_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 可以看到，起始的 cost 還蠻高，mse 到 25.  \n",
    "* 接著，我們做 backward：  \n",
    "  * 取得 gradient. \n",
    "  * 利用 gradient，來更新參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.3259],\n",
      "        [ 5.6211]])\n",
      "tensor([-6.3185])\n"
     ]
    }
   ],
   "source": [
    "batch_cost.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 可以看到，對 cost 做 backward 後，他自動找到要算 gradient 的對象 (w, b)，並且，把對應的 gradient 塞進去  \n",
    "* 然後，可以用我們寫好的 sgd，來更新參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer([w, b], lr = 0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimate w: tensor([[ 0.0619],\n",
      "        [-0.1699]], requires_grad=True)\n",
      "true w: tensor([ 2.0000, -3.4000])\n",
      "mse of parameter estimation: 14.190025329589844\n"
     ]
    }
   ],
   "source": [
    "print(f\"estimate w: {w}\")\n",
    "print(f\"true w: {true_w}\")\n",
    "print(f\"mse of parameter estimation: {((w.reshape(-1)-true_w)**2).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimate b: tensor([0.1896], requires_grad=True)\n",
      "true b: 4.2\n",
      "mse of parameter estimation: 16.083666673612385\n"
     ]
    }
   ],
   "source": [
    "print(f\"estimate b: {b}\")\n",
    "print(f\"true b: {true_b}\")\n",
    "print(f\"mse of parameter estimation: {((b.item()-true_b)**2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 可以看到，經過一個 batch，w 和 b 的參數估計，都更接近真值了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 最後，這邊要注意一下：  \n",
    "  * 每次做完參數更新後，都會把參數對應的 gradient 歸 0。  \n",
    "  * 做法是： `w.grad.zero_()` or `b.grad.zero_()`. \n",
    "  * 要這麼做的原因是，當我們做 `my_cost.backward()` 時，他背後做的事情是：  \n",
    "    * 先依據目前的 cost，算出每個 trainiable variable 的 gradient. \n",
    "    * 把這個 gradient，\"累加\" 到目前 trainable variable 的 gradient 上. \n",
    "  * 所以，如果你不把 trainable variable 的 gradient 歸 0，那新的 gradient 變成 新+舊 的 gradient，整個歪掉. \n",
    "  * 那 pytorch 慣例的寫法，不是在更新完 weight 後，清空 gradient。而是在算 backward 前，清空 gradient. \n",
    "  * 雖然看起來這兩種做法，看似等價，但如果你是依據現有的 model weight 延續做 retrain 的話，那你就不能確認一開始的 gradient 是 0。所以總是先歸0，再做 backward，就變成是 pytorch 的 convention. \n",
    "  * 這在等一下的 3 個 epoch 範例就可以看到例子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 走 3 個 epoch 看看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "origin_pos": 30,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "lr = 0.03\n",
    "num_epochs = 3\n",
    "\n",
    "# 初始化參數\n",
    "w = torch.normal(0, 0.01, size=(2,1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "params = (w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "origin_pos": 32,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.00040435069240629673\n",
      "epoch 2, loss 0.00010177450167248026\n",
      "epoch 3, loss 0.00010079665662487969\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    custom_data_iter = my_data_iter(10, features, labels)\n",
    "    for X, y in custom_data_iter:\n",
    "        # forward step\n",
    "        y_hat = model(X, w, b)\n",
    "        batch_cost = loss(y_hat, y)\n",
    "        \n",
    "        # 清空 gradient\n",
    "        for param in params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "        \n",
    "        # backward step\n",
    "        batch_cost.backward()\n",
    "        optimizer([w, b], lr = 0.03)  # 使用参数的梯度更新参數\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        y_hat_all = model(features, w, b)\n",
    "        epoch_cost = loss(y_hat_all, labels)\n",
    "        print(f'epoch {epoch +1}, loss {epoch_cost}')\n",
    "        #print(f'epoch {epoch + 1}, loss {float(epoch_cost):f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 可以看到，參數估計結果，和真值非常接近"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimate w: tensor([[ 2.0004],\n",
      "        [-3.3999]], requires_grad=True)\n",
      "true w: tensor([ 2.0000, -3.4000])\n",
      "mse of parameter estimation: 1.8516522004574654e-07\n"
     ]
    }
   ],
   "source": [
    "print(f\"estimate w: {w}\")\n",
    "print(f\"true w: {true_w}\")\n",
    "print(f\"mse of parameter estimation: {((w.reshape(-1)-true_w)**2).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimate b: tensor([4.1996], requires_grad=True)\n",
      "true b: 4.2\n",
      "mse of parameter estimation: 1.8969775737802604e-07\n"
     ]
    }
   ],
   "source": [
    "print(f\"estimate b: {b}\")\n",
    "print(f\"true b: {true_b}\")\n",
    "print(f\"mse of parameter estimation: {((b.item()-true_b)**2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 內建 function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "dataset = TensorDataset(features, labels)\n",
    "data_iter = DataLoader(dataset, batch_size = 10, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 驗證一下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8684,  1.8096],\n",
      "        [-0.4814, -0.4445],\n",
      "        [-0.3560, -1.1624],\n",
      "        [-1.6850,  0.9945],\n",
      "        [ 2.2541, -0.8971],\n",
      "        [-0.5724, -0.4106],\n",
      "        [-4.1179, -0.5625],\n",
      "        [-0.4651, -1.3560],\n",
      "        [-0.3070,  0.7921],\n",
      "        [ 0.4441, -0.7416]]) \n",
      " tensor([[-0.2113],\n",
      "        [ 4.7432],\n",
      "        [ 7.4426],\n",
      "        [-2.5434],\n",
      "        [11.7608],\n",
      "        [ 4.4673],\n",
      "        [-2.1368],\n",
      "        [ 7.8829],\n",
      "        [ 0.8922],\n",
      "        [ 7.6340]])\n"
     ]
    }
   ],
   "source": [
    "for X, y in data_iter:\n",
    "    print(X, '\\n', y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定義模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 1) # input 2, output 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 我們可以看一下這個模型的架構："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.state_dict of Sequential(\n",
       "  (0): Linear(in_features=2, out_features=1, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 可以看到，他只有一層，這一層是 linear 層。\n",
    "* 所以我們可以取出這一層來看看："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2, out_features=1, bias=True)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 取裡面的 weight 和 bias 來看看："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1620, -0.4444]], requires_grad=True)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.5005], requires_grad=True)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 單純把數值取出來："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1620, -0.4444]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5005])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 一次取全部的參數："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x132cbb190>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 是個 generator，那就用 for 迴圈去看總共有哪些東西："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1620, -0.4444]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.5005], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 發現就是 w 和 b  \n",
    "* 所以，等等要餵給 optimizer 的參數，就是 `model.parameters()`. \n",
    "* 然後，如果想看 w 和 b 的估計狀況，要用 `model[0].weight.data` 和 `model[0].bias.data`. \n",
    "* 如果想看 w 和 b 的 gradient，要用 `model[0].weight.grad` 和 `model[0].bias.grad`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 如果要取得 w 和 b 的數值，那要這麼做："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 走一個 batch 看看發生什麼事"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7370e+00,  2.2797e+00],\n",
      "        [-1.3517e+00,  6.1767e-01],\n",
      "        [-8.5405e-02, -4.3337e-01],\n",
      "        [-4.4443e-01, -2.2241e-01],\n",
      "        [ 4.1111e-01, -7.4746e-01],\n",
      "        [ 1.6160e+00,  2.0642e+00],\n",
      "        [-2.3694e+00, -4.2574e-01],\n",
      "        [-1.6769e-03, -1.1355e+00],\n",
      "        [ 7.4750e-01,  3.2721e-01],\n",
      "        [ 7.4800e-01,  9.3969e-01]])\n",
      "tensor([[-7.0367],\n",
      "        [-0.6083],\n",
      "        [ 5.5124],\n",
      "        [ 4.0628],\n",
      "        [ 7.5595],\n",
      "        [ 0.3955],\n",
      "        [ 0.9199],\n",
      "        [ 8.0442],\n",
      "        [ 4.5495],\n",
      "        [ 2.5009]])\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "X, y = next(iter(data_iter))\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 看一下初始參數和真值的差距"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_est = model[0].weight.data\n",
    "b_est = model[0].bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimate w: tensor([[ 0.1620, -0.4444]])\n",
      "true w: tensor([ 2.0000, -3.4000])\n",
      "mse of parameter estimation: 12.113733291625977\n"
     ]
    }
   ],
   "source": [
    "print(f\"estimate w: {w_est}\")\n",
    "print(f\"true w: {true_w}\")\n",
    "print(f\"mse of parameter estimation: {((w_est.reshape(-1)-true_w)**2).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimate b: tensor([-0.5005])\n",
      "true b: 4.2\n",
      "mse of parameter estimation: 22.09457495294072\n"
     ]
    }
   ],
   "source": [
    "print(f\"estimate b: {b_est}\")\n",
    "print(f\"true b: {true_b}\")\n",
    "print(f\"mse of parameter estimation: {((b_est.item()-true_b)**2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 此時，這些參數對應的 gradient 也都還是 None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model[0].weight.grad)\n",
    "print(model[0].bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 做第一次 forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(24.7222, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "y_hat = model(X)\n",
    "batch_cost = loss(y_hat, y)\n",
    "print(batch_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_cost.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 此時，gradient 已經算出來了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.8292,  4.5661]])\n",
      "tensor([-6.5510])\n"
     ]
    }
   ],
   "source": [
    "print(model[0].weight.grad)\n",
    "print(model[0].bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 更新參數 by optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 看一下參數更新的狀況，是否和真值更近了一些："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_est = model[0].weight.data\n",
    "b_est = model[0].bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimate w: tensor([[ 0.2468, -0.5814]])\n",
      "true w: tensor([ 2.0000, -3.4000])\n",
      "mse of parameter estimation: 11.017970085144043\n"
     ]
    }
   ],
   "source": [
    "print(f\"estimate w: {w_est}\")\n",
    "print(f\"true w: {true_w}\")\n",
    "print(f\"mse of parameter estimation: {((w_est.reshape(-1)-true_w)**2).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimate b: tensor([-0.3040])\n",
      "true b: 4.2\n",
      "mse of parameter estimation: 20.28563309076185\n"
     ]
    }
   ],
   "source": [
    "print(f\"estimate b: {b_est}\")\n",
    "print(f\"true b: {true_b}\")\n",
    "print(f\"mse of parameter estimation: {((b_est.item()-true_b)**2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 沒錯～ 估計得更好了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 走 3 個 epoch 看看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.000374063674826175\n",
      "epoch 2, loss 0.00010071097494801506\n",
      "epoch 3, loss 0.0001010773194138892\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        # forward\n",
    "        y_hat = model(X)\n",
    "        batch_cost = loss(y_hat ,y)\n",
    "        \n",
    "        # 清 gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # backward\n",
    "        batch_cost.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # training epoch loss\n",
    "    epoch_cost = loss(model(features), labels)\n",
    "    print(f'epoch {epoch + 1}, loss {epoch_cost}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 可以看一下，參數估計結果和真值非常接近"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_est = model[0].weight.data\n",
    "b_est = model[0].bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimate w: tensor([[ 2.0005, -3.4002]])\n",
      "true w: tensor([ 2.0000, -3.4000])\n",
      "mse of parameter estimation: 3.2639093205943936e-07\n"
     ]
    }
   ],
   "source": [
    "print(f\"estimate w: {w_est}\")\n",
    "print(f\"true w: {true_w}\")\n",
    "print(f\"mse of parameter estimation: {((w_est.reshape(-1)-true_w)**2).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimate b: tensor([4.1997])\n",
      "true b: 4.2\n",
      "mse of parameter estimation: 1.216326290888321e-07\n"
     ]
    }
   ],
   "source": [
    "print(f\"estimate b: {b_est}\")\n",
    "print(f\"true b: {true_b}\")\n",
    "print(f\"mse of parameter estimation: {((b_est.item()-true_b)**2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 內建 function vs 自己寫的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 最大的差別，應該是在 data iterator\n",
    "* 自己寫的 data iterator，他跑完後就沒了，所以寫法必須寫成這樣：  \n",
    "\n",
    "```python\n",
    "for epoch in range(num_epochs):\n",
    "    # 每個 epoch 下，都要再做出一個新的 iterator\n",
    "    custom_data_iter = my_data_iter(10, features, labels)\n",
    "    for X, y in custom_data_iter:\n",
    "        ...\n",
    "```\n",
    "\n",
    "* 但如果是用內建 function 的 iterator，那就做一次就好，因為它會自動幫你重新實例化：\n",
    "\n",
    "\n",
    "```python\n",
    "dataset = TensorDataset(features, labels)\n",
    "data_iter = DataLoader(dataset, batch_size = 10, shuffle = True) # 就做這麼一次就好\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}