{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 整理來源：  \n",
    "  * 修改自 hands on 那本的 **Chapter 12 – Custom Models and Training with TensorFlow** 。 github 的 notebook 名稱為 **12_custom_models_and_training_with_tensorflow.ipynb**\n",
    "  * 自訂 metric 整理自\n",
    "  * 自訂 callback 整理自. \n",
    "  * 自訂 fit 整理自"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 7) # This project requires Python 3.7 or above\n",
    "\n",
    "from packaging import version\n",
    "import tensorflow as tf\n",
    "# assert version.parse(tf.__version__) >= version.parse(\"2.8.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 拿 california housing 的資料來當範例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* features 有 8 個，y 是 numeric，所以是回歸問題"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11610, 8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.442],\n",
       "       [1.687],\n",
       "       [1.621],\n",
       "       ...,\n",
       "       [0.68 ],\n",
       "       [0.613],\n",
       "       [1.97 ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自訂 loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 不帶參數的 loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 對於回歸問題，最常用的 loss 是 mse\n",
    "* 但 mse 對 outlier 敏感，所以，我今天想用一個自訂的 loss，叫 `huber loss`  \n",
    "* 他的定義是：  \n",
    "  * abs difference of y_true & y_pred < 1 時，用正常的 square error loss / 2\n",
    "  * abs diff >= 1 時，改成用線性的 loss (i.e. abs diff - 0.5). \n",
    "* 那在定義的時候，就是定義一個 function 如下： (特別注意，都要用 tf 的 function 來定義，這樣之後才能順利轉成 graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_fn(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < 1\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss  = tf.abs(error) - 0.5\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 看一下這個 loss 的長相："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x252 with 1 Axes>"
      ]
     },
     "metadata": {
      "filenames": {
       "image/png": "/Volumes/GoogleDrive/我的雲端硬碟/0. codepool_python/python_dl/mybook/_build/jupyter_execute/hands_on_ml3/tf_customization_18_0.png"
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extra code – shows what the Huber loss looks like\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 3.5))\n",
    "z = np.linspace(-4, 4, 200)\n",
    "z_center = np.linspace(-1, 1, 200)\n",
    "plt.plot(z, huber_fn(0, z), \"b-\", linewidth=2, label=\"huber($z$)\")\n",
    "plt.plot(z, z ** 2 / 2, \"r:\", linewidth=1)\n",
    "plt.plot(z_center, z_center ** 2 / 2, \"r\", linewidth=2)\n",
    "plt.plot([-1, -1], [0, huber_fn(0., -1.)], \"k--\")\n",
    "plt.plot([1, 1], [0, huber_fn(0., 1.)], \"k--\")\n",
    "plt.gca().axhline(y=0, color='k')\n",
    "plt.gca().axvline(x=0, color='k')\n",
    "plt.text(2.1, 3.5, r\"$\\frac{1}{2}z^2$\", color=\"r\", fontsize=15)\n",
    "plt.text(3.0, 2.2, r\"$|z| - \\frac{1}{2}$\", color=\"b\", fontsize=15)\n",
    "plt.axis([-4, 4, 0, 4])\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"$z$\")\n",
    "plt.legend(fontsize=14)\n",
    "plt.title(\"Huber loss\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 現在，來 build 一個 model，用這個我們自訂的 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'huber_fn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-c958c6295acc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m model.compile(\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhuber_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# 就這樣塞進來就好，就是這麼簡單!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nadam\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mae\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'huber_fn' is not defined"
     ]
    }
   ],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                          input_shape=input_shape),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=huber_fn, # 就這樣塞進來就好，就是這麼簡單!\n",
    "    optimizer=\"nadam\", \n",
    "    metrics=[\"mae\"]\n",
    ")\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 照常做法，把 model 存起來，之後可以用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model_with_a_custom_loss/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"my_model_with_a_custom_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 讀檔時，必須把 custom_objects 寫入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"my_model_with_a_custom_loss\",\n",
    "                                   custom_objects={\"huber_fn\": huber_fn})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 這樣，就可以繼續 retrain 了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.1904 - mean_absolute_error: 0.4699 - val_loss: 0.2363 - val_mean_absolute_error: 0.5045\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 975us/step - loss: 0.1773 - mean_absolute_error: 0.4514 - val_loss: 0.2182 - val_mean_absolute_error: 0.4884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14fc733a0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 帶參數的 loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 那如果我今天的 loss function 帶有參數，例如，剛剛的 huber func，我不希望是定死的 abs diff < 1，而是 abs diff < threshold\n",
    "* 那這時，就必須用 class 的寫法，且繼承自 `tf.keras.losses.Loss`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss  = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 這邊幾個重點講一下：  \n",
    "  * init 裡面，就放了我要下的參數 `threshold`，以及其他之後想加的參數. \n",
    "  * call 裡面，就是原本 loss function 的寫法  \n",
    "  * get_config 裡面，是把這次有用到的參數寫進去 (e.g. \"threshold\": self.threshold)，這樣做的用意是，如果你 model 訓練完，存成 .h5 檔時，他會知道你這個模型當初用的 loss 的 threshold 是多少。至於他為啥知道？是因為在 load_model 的時候，他會去 call HuberLoss 裡面的 get_config() 方法 (這個方法繼承自 `tf.keras.losses.Loss`，所以我剛剛自己定義的 HuberLoss 沒明寫這個 method)，然後他從 get_config 就能知道我用的 threshold 是多少了. \n",
    "* 那 training 時，就這樣做："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 0s 988us/step - loss: 0.4997 - mae: 0.7514 - val_loss: 0.5202 - val_mae: 0.6936\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 788us/step - loss: 0.2781 - mae: 0.5435 - val_loss: 0.3794 - val_mae: 0.5651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14f3e7f10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                          input_shape=input_shape),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(loss=HuberLoss(2.), optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 存檔時，就直接存："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/hanklee/.pyenv/versions/3.8.0/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /Users/hanklee/.pyenv/versions/3.8.0/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: my_model_with_a_custom_loss_class/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"my_model_with_a_custom_loss_class\")  # extra code – saving works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 之後，要讀檔，繼續往下 train 時，就是在定義一次 loss class，然後讀檔："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 也許在別的 python 環境，要 re-train，那就得在定義一次之前用的 loss 的 class\n",
    "\n",
    "class HuberLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss  = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}\n",
    "\n",
    "# 把 model load 回來的時候，只要在 custom_objects 放入 key-value\n",
    "# 那他就會去之前存好的 model 的 config 檔中，找到 \"HuberLoss\" 這個 key，然後叫出之前你存好的參數： threshold，然後用在你寫的 HuberLoss 物件中\n",
    "model = tf.keras.models.load_model(\"my_model_with_a_custom_loss_class\",\n",
    "                                   custom_objects={\"HuberLoss\": HuberLoss})\n",
    "# 驗證一下，他有記得 threshold\n",
    "model.loss.threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 最後，直接 train 就可以了，不需要再 compile 了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 0s 983us/step - loss: 0.2206 - mean_absolute_error: 0.4783 - val_loss: 0.3241 - val_mean_absolute_error: 0.5093\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 801us/step - loss: 0.2018 - mean_absolute_error: 0.4574 - val_loss: 0.2909 - val_mean_absolute_error: 0.4934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14f5d68b0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自訂 activation, initialze, regularize functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 不帶參數版本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 自己把這些函數寫一寫，只要記得兩個原則：  \n",
    "  * 用 tf 的 function 來寫 (e.g. 不要用 np.sqrt(), 要用 tf.sqrt())  \n",
    "  * input 都是 tensor (tf 版的 n-dim array)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自訂 activaton function\n",
    "def my_softplus(z):\n",
    "    return tf.math.log(1.0 + tf.exp(z))\n",
    "\n",
    "# 自訂 initializer\n",
    "def my_glorot_initializer(shape, dtype=tf.float32):\n",
    "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "\n",
    "# 自訂 l1-regularize method\n",
    "def my_l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
    "\n",
    "# 自訂約束函數，來確保全中都是正的\n",
    "def my_positive_weights(weights):  # return value is just tf.nn.relu(weights)\n",
    "    return tf.where(weights < 0., tf.zeros_like(weights), weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 使用時，就是放到 layer 裡面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = tf.keras.layers.Dense(1, activation=my_softplus,\n",
    "                              kernel_initializer=my_glorot_initializer,\n",
    "                              kernel_regularizer=my_l1_regularizer,\n",
    "                              kernel_constraint=my_positive_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* training 時就照 train，照 save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.1668 - mae: 0.7430 - val_loss: inf - val_mae: inf\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 844us/step - loss: 0.7359 - mae: 0.5977 - val_loss: 2.6252 - val_mae: 0.5870\n",
      "WARNING:tensorflow:From /Users/hanklee/.pyenv/versions/3.8.0/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /Users/hanklee/.pyenv/versions/3.8.0/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: my_model_with_many_custom_parts/assets\n"
     ]
    }
   ],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                          input_shape=input_shape),\n",
    "    tf.keras.layers.Dense(1, activation=my_softplus,\n",
    "                          kernel_initializer=my_glorot_initializer,\n",
    "                          kernel_regularizer=my_l1_regularizer,\n",
    "                          kernel_constraint=my_positive_weights)\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.save(\"my_model_with_many_custom_parts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 之後，要 load model時，就像之前一樣，加入 custom_objects 來讀："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5646 - mean_absolute_error: 0.5293 - val_loss: 0.9063 - val_mean_absolute_error: 0.5070\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 839us/step - loss: 0.4981 - mean_absolute_error: 0.4975 - val_loss: 0.7695 - val_mean_absolute_error: 0.4918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14c9e1100>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\n",
    "    \"my_model_with_many_custom_parts\",\n",
    "    custom_objects={\n",
    "       \"my_l1_regularizer\": my_l1_regularizer,\n",
    "       \"my_positive_weights\": my_positive_weights,\n",
    "       \"my_glorot_initializer\": my_glorot_initializer,\n",
    "       \"my_softplus\": my_softplus,\n",
    "    }\n",
    ")\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 帶參數的版本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 如果這些客製化的函數，是帶有額外參數的，那就要用 class 的寫法，且要繼承適當的 class，例如：  \n",
    "  * `tf.keras.regularizers.Regularizer`. \n",
    "  * `tf.keras.constraints.Constraint`. \n",
    "  * `tf.keras.initializers.Initializer`. \n",
    "* 底下以 regularizer 來舉例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyL1Regularizer(tf.keras.regularizers.Regularizer):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "\n",
    "    def __call__(self, weights):\n",
    "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"factor\": self.factor}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* fit 的時候 as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.1668 - mae: 0.7430 - val_loss: inf - val_mae: inf\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 905us/step - loss: 0.7359 - mae: 0.5977 - val_loss: 2.6252 - val_mae: 0.5870\n",
      "INFO:tensorflow:Assets written to: my_model_with_many_custom_parts/assets\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                          input_shape=input_shape),\n",
    "    tf.keras.layers.Dense(1, activation=my_softplus,\n",
    "                          kernel_regularizer=MyL1Regularizer(0.01),\n",
    "                          kernel_constraint=my_positive_weights,\n",
    "                          kernel_initializer=my_glorot_initializer),\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.save(\"my_model_with_many_custom_parts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* load 的時候，只要把 class 塞進去就好，他會自己 call `get_config`，取回當初使用的 factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5646 - mean_absolute_error: 0.5293 - val_loss: 0.9063 - val_mean_absolute_error: 0.5070\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4981 - mean_absolute_error: 0.4975 - val_loss: 0.7695 - val_mean_absolute_error: 0.4918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14ce93a90>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\n",
    "    \"my_model_with_many_custom_parts\",\n",
    "    custom_objects={\n",
    "       \"MyL1Regularizer\": MyL1Regularizer,\n",
    "       \"my_positive_weights\": my_positive_weights,\n",
    "       \"my_glorot_initializer\": my_glorot_initializer,\n",
    "       \"my_softplus\": my_softplus,\n",
    "    }\n",
    ")\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自訂 metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 直接講 best practice，就是要去繼承 `tf.keras.metrics.Metric` 這個 class，並且，除了 init 以外，還要定義 3 個 method，以及 get_config:\n",
    "  * `__init__`: 這個建構子裡面，要放最終去計算 metric 所需的元件。舉例來說，要算 accuracy，那你需要 (正確分類數 / 總數)。那最一開始就會 initialize 這兩個 tf.variable，並 initialize 為 0  \n",
    "  * `update_state()`，這是每一個 batch 跑完時，要更新目前的 state。舉例來說，如果你最終要算 accuracy，那每個 batch 結束時，就是把正確分類數，加到剛剛的正確分類variable裡面; 把batch總數，加到剛剛的總數 variable 裡、。\n",
    "  * `result()`： 計算 metric 的邏輯放在這裡。以 accuracy，這邊就會用剛剛訂的 正確分類數 variable，和總數 variable 去做 正確分類/總數 的計算。  \n",
    "  * `reset_state()`: 整個 epoch 結束後，我們會把所有剛剛計算的東西歸0，因為下一個 epoch 全部重來。\n",
    "  * `get_config()`: 方便存檔後，load進來，還保有 init 時定義的參數資訊\n",
    "* 現在，先來寫個自己的 accuracy，並和現有的 accuracy metric 比較，看寫得對不對"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### my_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAccuracy(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"my_accuracy\", dtype=\"float32\", threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, dtype=dtype, **kwargs)\n",
    "        self.threshold = 0.5\n",
    "        self.true_decision = self.add_weight(\n",
    "            name = \"true_decision\", dtype = dtype, initializer = \"zeros\"\n",
    "        )\n",
    "        self.total_number = self.add_weight(\n",
    "            name = \"total_number\", dtype = dtype, initializer = \"zeros\"\n",
    "        )\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.math.greater_equal(y_pred, self.threshold) # 此時為 tf.bool 的 type, 例如 [True, False, ..., True]\n",
    "        y_pred = tf.cast(y_pred, tf.int64) # 轉成 [1, 0, ..., 1]\n",
    "        \n",
    "        true_decision = tf.cast(tf.math.equal(y_true, y_pred), self.dtype)\n",
    "        total_number = tf.cast(tf.size(y_true), self.dtype)\n",
    "\n",
    "        self.true_decision.assign_add(tf.reduce_sum(true_decision))\n",
    "        self.total_number.assign_add(total_number)\n",
    "\n",
    "    def result(self):\n",
    "        if self.total_number == 0:\n",
    "            accuracy = tf.cast(0, tf.float32)\n",
    "        else:\n",
    "            accuracy = self.true_decision / self.total_number\n",
    "        return accuracy\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.true_decision.assign(0)\n",
    "        self.total_number.assign(0)\n",
    "        \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 來測試一下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- initialize result -----\n",
      "tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "----- end of first batch -----\n",
      "tf.Tensor(0.6, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6, shape=(), dtype=float32)\n",
      "----- end of second batch -----\n",
      "tf.Tensor(0.5, shape=(), dtype=float32)\n",
      "tf.Tensor(0.5, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "official_accuracy = tf.keras.metrics.BinaryAccuracy()\n",
    "my_accuracy = MyAccuracy()\n",
    "\n",
    "# 起始後，目前的結果\n",
    "print(\"----- initialize result -----\")\n",
    "print(official_accuracy.result())\n",
    "print(my_accuracy.result())\n",
    "\n",
    "# 第一批資料做完後的結果 (3/5)\n",
    "official_accuracy.update_state([[1, 0, 1, 1, 0]], [[0.8, 0.3, 0.6, 0.4, 0.6]])\n",
    "my_accuracy.update_state([[1, 0, 1, 1, 0]], [[0.8, 0.3, 0.6, 0.4, 0.6]])\n",
    "print(\"----- end of first batch -----\")\n",
    "print(official_accuracy.result())\n",
    "print(my_accuracy.result())\n",
    "\n",
    "# 第二批資料做完後的結果 (2/5 加到原本的 state，變成 (3+2)/(5+5) = 0.5)\n",
    "official_accuracy.update_state([[1, 1, 1, 0, 0]], [[0.8, 0.3, 0.4, 0.2, 0.6]])\n",
    "my_accuracy.update_state([[1, 1, 1, 0, 0]], [[0.8, 0.3, 0.4, 0.2, 0.6]])\n",
    "print(\"----- end of second batch -----\")\n",
    "print(official_accuracy.result())\n",
    "print(my_accuracy.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 讚啦！ 那就實際來使用這個 metric 吧："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "\n",
    "# x_train 是 tabular data, n = 12, p = 1 (僅 1 個 feature)\n",
    "x_train = np.array([\n",
    "    [1],\n",
    "    [2],\n",
    "    [3],\n",
    "    [4],\n",
    "    [5],\n",
    "    [6],\n",
    "    [7],\n",
    "    [8],\n",
    "    [9],\n",
    "    [10],\n",
    "    [11],\n",
    "    [12]\n",
    "])\n",
    "y_train = np.array([\n",
    "    [0], \n",
    "    [0], \n",
    "    [1], \n",
    "    [0], \n",
    "    [1], \n",
    "    [1], \n",
    "    [0], \n",
    "    [1], \n",
    "    [1], \n",
    "    [1], \n",
    "    [1], \n",
    "    [1]\n",
    "])\n",
    "ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)\n",
    ").shuffle(12).batch(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Step: 0\n",
      "Total running accuracy so far: 0.333\n",
      "Epoch: 0 Step: 1\n",
      "Total running accuracy so far: 0.500\n",
      "Epoch: 0 Step: 2\n",
      "Total running accuracy so far: 0.444\n",
      "Epoch: 0 Step: 3\n",
      "Total running accuracy so far: 0.333\n",
      "Epoch: 1 Step: 0\n",
      "Total running accuracy so far: 0.000\n",
      "Epoch: 1 Step: 1\n",
      "Total running accuracy so far: 0.333\n",
      "Epoch: 1 Step: 2\n",
      "Total running accuracy so far: 0.222\n",
      "Epoch: 1 Step: 3\n",
      "Total running accuracy so far: 0.333\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a metric object\n",
    "my_accuracy = MyAccuracy()\n",
    "\n",
    "# Prepare our layer, loss, and optimizer.\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(3, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1, activation = \"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "for epoch in range(2):\n",
    "    # Iterate over the batches of a dataset.\n",
    "    for step, (x, y) in enumerate(ds):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x)\n",
    "            # Compute the loss value for this batch.\n",
    "            loss_value = loss_fn(y, logits)\n",
    "\n",
    "        # Update the state of the `accuracy` metric.\n",
    "        my_accuracy.update_state(y, logits)\n",
    "\n",
    "        # Update the weights of the model to minimize the loss value.\n",
    "        gradients = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "\n",
    "        # Logging the current accuracy value so far.\n",
    "        print(\"Epoch:\", epoch, \"Step:\", step)\n",
    "        print(\"Total running accuracy so far: %.3f\" % my_accuracy.result())\n",
    "            \n",
    "\n",
    "    # Reset the metric's state at the end of an epoch\n",
    "    my_accuracy.reset_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 或是，更簡單一點，直接用 fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.5993 - my_accuracy: 0.6667 - binary_accuracy: 0.6667\n",
      "Epoch 2/2\n",
      "4/4 [==============================] - 0s 976us/step - loss: 0.5957 - my_accuracy: 0.6667 - binary_accuracy: 0.6667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14e99c670>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate a metric object\n",
    "my_accuracy = MyAccuracy()\n",
    "official_accuracy = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "# Prepare our layer, loss, and optimizer.\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(3, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1, activation = \"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "model.compile(\n",
    "    loss = loss_fn,\n",
    "    optimizer = optimizer,\n",
    "    metrics = [my_accuracy, official_accuracy]\n",
    ")\n",
    "\n",
    "model.fit(ds, epochs = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 接著，來實做一下 F1 score (取自: https://keras.io/getting_started/intro_to_keras_for_researchers/#keeping-track-of-training-metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"f1_score\", dtype=\"float32\", threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, dtype=dtype, **kwargs)\n",
    "        self.threshold = 0.5\n",
    "        self.true_positives = self.add_weight(\n",
    "            name=\"tp\", dtype=dtype, initializer=\"zeros\"\n",
    "        )\n",
    "        self.false_positives = self.add_weight(\n",
    "            name=\"fp\", dtype=dtype, initializer=\"zeros\"\n",
    "        )\n",
    "        self.false_negatives = self.add_weight(\n",
    "            name=\"fn\", dtype=dtype, initializer=\"zeros\"\n",
    "        )\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.math.greater_equal(y_pred, self.threshold)\n",
    "        y_true = tf.cast(y_true, tf.bool)\n",
    "        y_pred = tf.cast(y_pred, tf.bool)\n",
    "\n",
    "        true_positives = tf.cast(y_true & y_pred, self.dtype)\n",
    "        false_positives = tf.cast(~y_true & y_pred, self.dtype)\n",
    "        false_negatives = tf.cast(y_true & ~y_pred, self.dtype)\n",
    "\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, self.dtype)\n",
    "            true_positives *= sample_weight\n",
    "            false_positives *= sample_weight\n",
    "            false_negatives *= sample_weight\n",
    "\n",
    "        self.true_positives.assign_add(tf.reduce_sum(true_positives))\n",
    "        self.false_positives.assign_add(tf.reduce_sum(false_positives))\n",
    "        self.false_negatives.assign_add(tf.reduce_sum(false_negatives))\n",
    "\n",
    "    def result(self):\n",
    "        precision = self.true_positives / (self.true_positives + self.false_positives)\n",
    "        recall = self.true_positives / (self.true_positives + self.false_negatives)\n",
    "        return precision * recall * 2.0 / (precision + recall)\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.true_positives.assign(0)\n",
    "        self.false_positives.assign(0)\n",
    "        self.false_negatives.assign(0)\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.6981 - my_accuracy: 0.2500 - binary_accuracy: 0.2500 - f1_score: nan - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 2/2\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.6925 - my_accuracy: 0.6667 - binary_accuracy: 0.6667 - f1_score: 0.8000 - precision: 0.6667 - recall: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x150ba67c0>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate a metric object\n",
    "my_accuracy = MyAccuracy()\n",
    "official_accuracy = tf.keras.metrics.BinaryAccuracy()\n",
    "official_precision = tf.keras.metrics.Precision(name = \"precision\")\n",
    "official_recall = tf.keras.metrics.Recall(name = \"recall\")\n",
    "\n",
    "my_f1_score = F1Score()\n",
    "\n",
    "# Prepare our layer, loss, and optimizer.\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(3, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1, activation = \"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "model.compile(\n",
    "    loss = loss_fn,\n",
    "    optimizer = optimizer,\n",
    "    metrics = [my_accuracy, official_accuracy, my_f1_score, official_precision, official_recall]\n",
    ")\n",
    "\n",
    "model.fit(ds, epochs = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 存檔時，就照樣存："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model_with_a_custom_metric/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"my_model_with_a_custom_metric\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 讀檔時，跟之前一樣，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"my_model_with_a_custom_metric\",\n",
    "                                   custom_objects={\"MyAccuracy\": MyAccuracy, \"F1Score\": F1Score})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 可以看到，之前存好的 metric 都在"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.metrics.Mean at 0x15100d700>,\n",
       " <__main__.MyAccuracy at 0x150ff55b0>,\n",
       " <tensorflow.python.keras.metrics.BinaryAccuracy at 0x151005ca0>,\n",
       " <__main__.F1Score at 0x150eea370>,\n",
       " <tensorflow.python.keras.metrics.Precision at 0x150e85820>,\n",
       " <tensorflow.python.keras.metrics.Recall at 0x15104abb0>]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model.metrics` contains the model's loss followed by the model's metric(s), so the `HuberMetric` is `model.metrics[-1]`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 參數也都記得："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print(model.metrics[1].threshold)\n",
    "print(model.metrics[3].threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 可以繼續 train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.6921 - my_accuracy: 0.6667 - binary_accuracy: 0.6667 - f1_score: 0.8000 - precision: 0.6667 - recall: 1.0000\n",
      "Epoch 2/2\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.6917 - my_accuracy: 0.6667 - binary_accuracy: 0.6667 - f1_score: 0.8000 - precision: 0.6667 - recall: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1509e58b0>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(ds, epochs = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自訂 Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 也參考官網這篇：https://www.tensorflow.org/guide/keras/custom_layers_and_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "exponential_layer = tf.keras.layers.Lambda(lambda x: tf.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.36787945, 1.        , 2.7182817 ], dtype=float32)>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extra code – like all layers, it can be used as a function:\n",
    "exponential_layer([-1., 0., 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding an exponential layer at the output of a regression model can be useful if the values to predict are positive and with very different scales (e.g., 0.001, 10., 10000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "363/363 [==============================] - 0s 845us/step - loss: 1.0631 - val_loss: 0.4457\n",
      "Epoch 2/5\n",
      "363/363 [==============================] - 0s 591us/step - loss: 0.4562 - val_loss: 0.3798\n",
      "Epoch 3/5\n",
      "363/363 [==============================] - 0s 585us/step - loss: 0.4029 - val_loss: 0.3548\n",
      "Epoch 4/5\n",
      "363/363 [==============================] - 0s 597us/step - loss: 0.3851 - val_loss: 0.3464\n",
      "Epoch 5/5\n",
      "363/363 [==============================] - 0s 582us/step - loss: 0.3708 - val_loss: 0.3449\n",
      "162/162 [==============================] - 0s 427us/step - loss: 0.3586\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3586341440677643"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\", input_shape=input_shape),\n",
    "    tf.keras.layers.Dense(1),\n",
    "    exponential_layer\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=\"sgd\")\n",
    "model.fit(X_train_scaled, y_train, epochs=5,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, it's often preferable to replace the targets with the logarithm of the targets (and use no activation function in the output layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDense(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            name=\"kernel\", shape=[batch_input_shape[-1], self.units],\n",
    "            initializer=\"he_normal\")\n",
    "        self.bias = self.add_weight(\n",
    "            name=\"bias\", shape=[self.units], initializer=\"zeros\")\n",
    "        super().build(batch_input_shape)  # must be at the end\n",
    "\n",
    "    def call(self, X):\n",
    "        return self.activation(X @ self.kernel + self.bias)\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"units\": self.units,\n",
    "                \"activation\": tf.keras.activations.serialize(self.activation)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 836us/step - loss: 2.8036 - val_loss: 2.9430\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 671us/step - loss: 0.7903 - val_loss: 1.3091\n",
      "162/162 [==============================] - 0s 426us/step - loss: 0.6557\n",
      "INFO:tensorflow:Assets written to: my_model_with_a_custom_layer/assets\n"
     ]
    }
   ],
   "source": [
    "# extra code – shows that a custom layer can be used normally\n",
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential([\n",
    "    MyDense(30, activation=\"relu\", input_shape=input_shape),\n",
    "    MyDense(1)\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.evaluate(X_test_scaled, y_test)\n",
    "model.save(\"my_model_with_a_custom_layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 892us/step - loss: 0.5665 - val_loss: 0.4506\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 692us/step - loss: 0.4502 - val_loss: 0.5153\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f848168da30>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extra code – shows how to load a model with a custom layer\n",
    "model = tf.keras.models.load_model(\"my_model_with_a_custom_layer\",\n",
    "                                   custom_objects={\"MyDense\": MyDense})\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMultiLayer(tf.keras.layers.Layer):\n",
    "    def call(self, X):\n",
    "        X1, X2 = X\n",
    "        print(\"X1.shape: \", X1.shape ,\" X2.shape: \", X2.shape)  # extra code\n",
    "        return X1 + X2, X1 * X2, X1 / X2\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        batch_input_shape1, batch_input_shape2 = batch_input_shape\n",
    "        return [batch_input_shape1, batch_input_shape1, batch_input_shape1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our custom layer can be called using the functional API like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1.shape:  (None, 2)  X2.shape:  (None, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'my_multi_layer_4')>,\n",
       " <KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'my_multi_layer_4')>,\n",
       " <KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'my_multi_layer_4')>)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extra code – tests MyMultiLayer with symbolic inputs\n",
    "inputs1 = tf.keras.layers.Input(shape=[2])\n",
    "inputs2 = tf.keras.layers.Input(shape=[2])\n",
    "MyMultiLayer()((inputs1, inputs2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `call()` method receives symbolic inputs, and it returns symbolic outputs. The shapes are only partially specified at this stage: we don't know the batch size, which is why the first dimension is `None`.\n",
    "\n",
    "We can also pass actual data to the custom layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1.shape:  (2, 2)  X2.shape:  (2, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       " array([[ 9., 18.],\n",
       "        [ 6., 10.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       " array([[18., 72.],\n",
       "        [ 8., 21.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       " array([[0.5      , 0.5      ],\n",
       "        [0.5      , 2.3333333]], dtype=float32)>)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extra code – tests MyMultiLayer with actual data \n",
    "X1, X2 = np.array([[3., 6.], [2., 7.]]), np.array([[6., 12.], [4., 3.]]) \n",
    "MyMultiLayer()((X1, X2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a layer with a different behavior during training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGaussianNoise(tf.keras.layers.Layer):\n",
    "    def __init__(self, stddev, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def call(self, X, training=None):\n",
    "        if training:\n",
    "            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\n",
    "            return X + noise\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return batch_input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a simple model that uses this custom layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 867us/step - loss: 2.1976 - val_loss: 26.5902\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 671us/step - loss: 1.4224 - val_loss: 19.3606\n",
      "162/162 [==============================] - 0s 423us/step - loss: 1.0180\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0180009603500366"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extra code – tests MyGaussianNoise\n",
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential([\n",
    "    MyGaussianNoise(stddev=1.0, input_shape=input_shape),\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自訂 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [tf.keras.layers.Dense(n_neurons, activation=\"relu\",\n",
    "                                             kernel_initializer=\"he_normal\")\n",
    "                       for _ in range(n_layers)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        return inputs + Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualRegressor(tf.keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = tf.keras.layers.Dense(30, activation=\"relu\",\n",
    "                                             kernel_initializer=\"he_normal\")\n",
    "        self.block1 = ResidualBlock(2, 30)\n",
    "        self.block2 = ResidualBlock(2, 30)\n",
    "        self.out = tf.keras.layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = self.hidden1(inputs)\n",
    "        for _ in range(1 + 3):\n",
    "            Z = self.block1(Z)\n",
    "        Z = self.block2(Z)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 810us/step - loss: 5.2455\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 807us/step - loss: 0.8515\n",
      "162/162 [==============================] - 0s 512us/step - loss: 0.6072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_19_layer_call_and_return_conditional_losses, dense_19_layer_call_fn, dense_20_layer_call_and_return_conditional_losses, dense_20_layer_call_fn, dense_21_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_custom_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_custom_model/assets\n"
     ]
    }
   ],
   "source": [
    "# extra code – shows that the model can be used normally\n",
    "tf.random.set_seed(42)\n",
    "model = ResidualRegressor(1)\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = model.fit(X_train_scaled, y_train, epochs=2)\n",
    "score = model.evaluate(X_test_scaled, y_test)\n",
    "model.save(\"my_custom_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 879us/step - loss: 0.7176\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 816us/step - loss: 0.5186\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.62953055],\n",
       "       [1.2767944 ],\n",
       "       [4.634055  ]], dtype=float32)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extra code – the model can be loaded and you can continue training or use it\n",
    "#              to make predictions\n",
    "model = tf.keras.models.load_model(\"my_custom_model\")\n",
    "history = model.fit(X_train_scaled, y_train, epochs=2)\n",
    "model.predict(X_test_scaled[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have defined the model using the sequential API instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "block1 = ResidualBlock(2, 30)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    block1, block1, block1, block1,\n",
    "    ResidualBlock(2, 30),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses and Metrics Based on Model Internals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: due to an issue introduced in TF 2.2 ([#46858](https://github.com/tensorflow/tensorflow/issues/46858)), `super().build()` fails. We can work around this issue by setting `self.built = True` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReconstructingRegressor(tf.keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [tf.keras.layers.Dense(30, activation=\"relu\",\n",
    "                                             kernel_initializer=\"he_normal\")\n",
    "                       for _ in range(5)]\n",
    "        self.out = tf.keras.layers.Dense(output_dim)\n",
    "        self.reconstruction_mean = tf.keras.metrics.Mean(\n",
    "            name=\"reconstruction_error\")\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        n_inputs = batch_input_shape[-1]\n",
    "        self.reconstruct = tf.keras.layers.Dense(n_inputs)\n",
    "        self.built = True  # WORKAROUND for super().build(batch_input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        reconstruction = self.reconstruct(Z)\n",
    "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
    "        self.add_loss(0.05 * recon_loss)\n",
    "        if training:\n",
    "            result = self.reconstruction_mean(recon_loss)\n",
    "            self.add_metric(result)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "363/363 [==============================] - 1s 820us/step - loss: 0.7640 - reconstruction_error: 1.2728\n",
      "Epoch 2/5\n",
      "363/363 [==============================] - 0s 809us/step - loss: 0.4584 - reconstruction_error: 0.6340\n",
      "Epoch 3/5\n",
      "363/363 [==============================] - 0s 786us/step - loss: 0.4211 - reconstruction_error: 0.4342\n",
      "Epoch 4/5\n",
      "363/363 [==============================] - 0s 745us/step - loss: 0.3753 - reconstruction_error: 0.3597\n",
      "Epoch 5/5\n",
      "363/363 [==============================] - 0s 772us/step - loss: 0.3618 - reconstruction_error: 0.2908\n"
     ]
    }
   ],
   "source": [
    "# extra code\n",
    "tf.random.set_seed(42)\n",
    "model = ReconstructingRegressor(1)\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = model.fit(X_train_scaled, y_train, epochs=5)\n",
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自訂 callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 整理自： https://keras.io/guides/writing_your_own_callbacks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自訂 fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Gradients Using Autodiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + 2 * w1 * w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.000003007075065"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1, w2 = 5, 3\n",
    "eps = 1e-6\n",
    "(f(w1 + eps, w2) - f(w1, w2)) / eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.000000003174137"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(f(w1, w2 + eps) - f(w1, w2)) / eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "gradients = tape.gradient(z, [w1, w2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians)\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "dz_dw1 = tape.gradient(z, w1)  # returns tensor 36.0\n",
    "try:\n",
    "    dz_dw2 = tape.gradient(z, w2)  # raises a RuntimeError!\n",
    "except RuntimeError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "dz_dw1 = tape.gradient(z, w1)  # returns tensor 36.0\n",
    "dz_dw2 = tape.gradient(z, w2)  # returns tensor 10.0, works fine now!\n",
    "del tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dz_dw1, dz_dw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1, c2 = tf.constant(5.), tf.constant(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(c1, c2)\n",
    "\n",
    "gradients = tape.gradient(z, [c1, c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(c1)\n",
    "    tape.watch(c2)\n",
    "    z = f(c1, c2)\n",
    "\n",
    "gradients = tape.gradient(z, [c1, c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=136.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=30.0>]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extra code – if given a vector, tape.gradient() will compute the gradient of\n",
    "#              the vector's sum.\n",
    "with tf.GradientTape() as tape:\n",
    "    z1 = f(w1, w2 + 2.)\n",
    "    z2 = f(w1, w2 + 5.)\n",
    "    z3 = f(w1, w2 + 7.)\n",
    "\n",
    "tape.gradient([z1, z2, z3], [w1, w2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=136.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=30.0>]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extra code – shows that we get the same result as the previous cell\n",
    "with tf.GradientTape() as tape:\n",
    "    z1 = f(w1, w2 + 2.)\n",
    "    z2 = f(w1, w2 + 5.)\n",
    "    z3 = f(w1, w2 + 7.)\n",
    "    z = z1 + z2 + z3\n",
    "\n",
    "tape.gradient(z, [w1, w2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – shows how to compute the jacobians and the hessians\n",
    "with tf.GradientTape(persistent=True) as hessian_tape:\n",
    "    with tf.GradientTape() as jacobian_tape:\n",
    "        z = f(w1, w2)\n",
    "    jacobians = jacobian_tape.gradient(z, [w1, w2])\n",
    "hessians = [hessian_tape.gradient(jacobian, [w1, w2])\n",
    "            for jacobian in jacobians]\n",
    "del hessian_tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacobians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<tf.Tensor: shape=(), dtype=float32, numpy=6.0>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=2.0>],\n",
       " [<tf.Tensor: shape=(), dtype=float32, numpy=2.0>, None]]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hessians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + tf.stop_gradient(2 * w1 * w2)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)  # same result as without stop_gradient()\n",
    "\n",
    "gradients = tape.gradient(z, [w1, w2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=30.0>, None]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=inf>]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable(1e-50)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = tf.sqrt(x)\n",
    "\n",
    "tape.gradient(z, [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=30.0>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.log(tf.exp(tf.constant(30., dtype=tf.float32)) + 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable([1.0e30])\n",
    "with tf.GradientTape() as tape:\n",
    "    z = my_softplus(x)\n",
    "\n",
    "tape.gradient(z, [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softplus(z):\n",
    "    return tf.math.log(1 + tf.exp(-tf.abs(z))) + tf.maximum(0., z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the proof that this equation is equal to log(1 + exp(_z_)):\n",
    "* softplus(_z_) = log(1 + exp(_z_))\n",
    "* softplus(_z_) = log(1 + exp(_z_)) - log(exp(_z_)) + log(exp(_z_)) ; **just adding and subtracting the same value**\n",
    "* softplus(_z_) = log\\[(1 + exp(_z_)) / exp(_z_)\\] + log(exp(_z_)) ; **since log(_a_) - log(_b_) = log(_a_ / _b_)**\n",
    "* softplus(_z_) = log\\[(1 + exp(_z_)) / exp(_z_)\\] + _z_ ; **since log(exp(_z_)) = _z_**\n",
    "* softplus(_z_) = log\\[1 / exp(_z_) + exp(_z_) / exp(_z_)\\] + _z_ ; **since (1 + _a_) / _b_ = 1 / _b_ + _a_ / _b_**\n",
    "* softplus(_z_) = log\\[exp(–_z_) + 1\\] + _z_ ; **since 1 / exp(_z_) = exp(–z), and exp(_z_) / exp(_z_) = 1**\n",
    "* softplus(_z_) = softplus(–_z_) + _z_ ; **we recognize the definition at the top, but with –_z_**\n",
    "* softplus(_z_) = softplus(–|_z_|) + max(0, _z_) ; **if you consider both cases, _z_ < 0 or _z_ ≥ 0, you will see that this works**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def my_softplus(z):\n",
    "    def my_softplus_gradients(grads):  # grads = backprop'ed from upper layers\n",
    "        return grads * (1 - 1 / (1 + tf.exp(z)))  # stable grads of softplus\n",
    "\n",
    "    result = tf.math.log(1 + tf.exp(-tf.abs(z))) + tf.maximum(0., z)\n",
    "    return result, my_softplus_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1000.], dtype=float32)>,\n",
       " [<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extra code – shows that the function is now stable, as well as its gradients\n",
    "x = tf.Variable([1000.])\n",
    "with tf.GradientTape() as tape:\n",
    "    z = my_softplus(x)\n",
    "\n",
    "z, tape.gradient(z, [x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自訂 Training Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)  # extra code – to ensure reproducibility\n",
    "l2_reg = tf.keras.regularizers.l2(0.05)\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                          kernel_regularizer=l2_reg),\n",
    "    tf.keras.layers.Dense(1, kernel_regularizer=l2_reg)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(X, y, batch_size=32):\n",
    "    idx = np.random.randint(len(X), size=batch_size)\n",
    "    return X[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_status_bar(step, total, loss, metrics=None):\n",
    "    metrics = \" - \".join([f\"{m.name}: {m.result():.4f}\"\n",
    "                          for m in [loss] + (metrics or [])])\n",
    "    end = \"\" if step < total else \"\\n\"\n",
    "    print(f\"\\r{step}/{total} - \" + metrics, end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "loss_fn = tf.keras.losses.mean_squared_error\n",
    "mean_loss = tf.keras.metrics.Mean()\n",
    "metrics = [tf.keras.metrics.MeanAbsoluteError()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "362/362 - mean: 0.6219 - mean_absolute_error: 0.4975\n",
      "Epoch 2/5\n",
      "362/362 - mean: 0.6272 - mean_absolute_error: 0.5049\n",
      "Epoch 3/5\n",
      "362/362 - mean: 0.6019 - mean_absolute_error: 0.4951\n",
      "Epoch 4/5\n",
      "362/362 - mean: 0.6088 - mean_absolute_error: 0.4971\n",
      "Epoch 5/5\n",
      "362/362 - mean: 0.6159 - mean_absolute_error: 0.5032\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(f\"Epoch {epoch}/{n_epochs}\")\n",
    "    for step in range(1, n_steps + 1):\n",
    "        X_batch, y_batch = random_batch(X_train_scaled, y_train)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch, training=True)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        # extra code – if your model has variable constraints\n",
    "        for variable in model.variables:\n",
    "            if variable.constraint is not None:\n",
    "                variable.assign(variable.constraint(variable))\n",
    "\n",
    "        mean_loss(loss)\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "\n",
    "        print_status_bar(step, n_steps, mean_loss, metrics)\n",
    "\n",
    "    for metric in [mean_loss] + metrics:\n",
    "        metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0425ac6b66024c7d83d98459be6f1811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "All epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fac07a118bd649158e28c73591809f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5:   0%|          | 0/362 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "162f6cd2e9b4491d9a3b1bc378990eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5:   0%|          | 0/362 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "116af880df174758bf744dc1fe5fa81e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5:   0%|          | 0/362 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20afe56af0b54d12be3dd84e29e9f0d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5:   0%|          | 0/362 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "662afff06af24ded83a719eabc1a8a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5:   0%|          | 0/362 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extra code – shows how to use the tqdm package to display nice progress bars\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "from collections import OrderedDict\n",
    "with trange(1, n_epochs + 1, desc=\"All epochs\") as epochs:\n",
    "    for epoch in epochs:\n",
    "        with trange(1, n_steps + 1, desc=f\"Epoch {epoch}/{n_epochs}\") as steps:\n",
    "            for step in steps:\n",
    "                X_batch, y_batch = random_batch(X_train_scaled, y_train)\n",
    "                with tf.GradientTape() as tape:\n",
    "                    y_pred = model(X_batch)\n",
    "                    main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "                    loss = tf.add_n([main_loss] + model.losses)\n",
    "\n",
    "                gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "                for variable in model.variables:\n",
    "                    if variable.constraint is not None:\n",
    "                        variable.assign(variable.constraint(variable))\n",
    "\n",
    "                status = OrderedDict()\n",
    "                mean_loss(loss)\n",
    "                status[\"loss\"] = mean_loss.result().numpy()\n",
    "                for metric in metrics:\n",
    "                    metric(y_batch, y_pred)\n",
    "                    status[metric.name] = metric.result().numpy()\n",
    "\n",
    "                steps.set_postfix(status)\n",
    "\n",
    "        for metric in [mean_loss] + metrics:\n",
    "            metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自訂 tensorboard 要記錄的東西"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cube(x):\n",
    "    return x ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=8.0>"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube(tf.constant(2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.def_function.Function at 0x7f83d1c22a90>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube = tf.function(cube)\n",
    "tf_cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=8>"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=8.0>"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube(tf.constant(2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def tf_cube(x):\n",
    "    return x ** 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** the rest of the code in this section is in appendix D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF Functions and Concrete Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConcreteFunction tf_cube(x) at 0x7F843148D910>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_function = tf_cube.get_concrete_function(tf.constant(2.0))\n",
    "concrete_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=8.0>"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_function(tf.constant(2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_function is tf_cube.get_concrete_function(tf.constant(2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Function Definitions and Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.framework.func_graph.FuncGraph at 0x7f8441219040>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_function.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'x' type=Placeholder>,\n",
       " <tf.Operation 'pow/y' type=Const>,\n",
       " <tf.Operation 'pow' type=Pow>,\n",
       " <tf.Operation 'Identity' type=Identity>]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ops = concrete_function.graph.get_operations()\n",
    "ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'x:0' shape=() dtype=float32>,\n",
       " <tf.Tensor 'pow/y:0' shape=() dtype=float32>]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pow_op = ops[2]\n",
    "list(pow_op.inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'pow:0' shape=() dtype=float32>]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pow_op.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Operation 'x' type=Placeholder>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_function.graph.get_operation_by_name('x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Identity:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_function.graph.get_tensor_by_name('Identity:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"__inference_tf_cube_3515915\"\n",
       "input_arg {\n",
       "  name: \"x\"\n",
       "  type: DT_FLOAT\n",
       "}\n",
       "output_arg {\n",
       "  name: \"identity\"\n",
       "  type: DT_FLOAT\n",
       "}"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_function.function_def.signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How TF Functions Trace Python Functions to Extract Their Computation Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def tf_cube(x):\n",
    "    print(f\"x = {x}\")\n",
    "    return x ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = Tensor(\"x:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "result = tf_cube(tf.constant(2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=8.0>"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = 2\n"
     ]
    }
   ],
   "source": [
    "result = tf_cube(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = 3\n"
     ]
    }
   ],
   "source": [
    "result = tf_cube(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = Tensor(\"x:0\", shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "result = tf_cube(tf.constant([[1., 2.]]))  # New shape: trace!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = Tensor(\"x:0\", shape=(2, 2), dtype=float32)\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function tf_cube at 0x7f84312f51f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function tf_cube at 0x7f84312f51f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "result = tf_cube(tf.constant([[3., 4.], [5., 6.]]))  # New shape: trace!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tf_cube(tf.constant([[7., 8.], [9., 10.]]))  # Same shape: no trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to specify a particular input signature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(input_signature=[tf.TensorSpec([None, 28, 28], tf.float32)])\n",
    "def shrink(images):\n",
    "    print(\"Tracing\", images)  # extra code to show when tracing happens\n",
    "    return images[:, ::2, ::2] # drop half the rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing Tensor(\"images:0\", shape=(None, 28, 28), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "img_batch_1 = tf.random.uniform(shape=[100, 28, 28])\n",
    "img_batch_2 = tf.random.uniform(shape=[50, 28, 28])\n",
    "preprocessed_images = shrink(img_batch_1)  # Works fine, traces the function\n",
    "preprocessed_images = shrink(img_batch_2)  # Works fine, same concrete function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python inputs incompatible with input_signature:\n",
      "  inputs: (\n",
      "    tf.Tensor(\n",
      "[[[0.7413678  0.62854624]\n",
      "  [0.01738465 0.3431449 ]]\n",
      "\n",
      " [[0.51063764 0.3777541 ]\n",
      "  [0.07321596 0.02137029]]], shape=(2, 2, 2), dtype=float32))\n",
      "  input_signature: (\n",
      "    TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name=None))\n"
     ]
    }
   ],
   "source": [
    "img_batch_3 = tf.random.uniform(shape=[2, 2, 2])\n",
    "try:\n",
    "    preprocessed_images = shrink(img_batch_3)  # ValueError! Incompatible inputs\n",
    "except ValueError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Autograph To Capture Control Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A \"static\" `for` loop using `range()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def add_10(x):\n",
    "    for i in range(10):\n",
    "        x += 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=15>"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_10(tf.constant(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'x' type=Placeholder>,\n",
       " <tf.Operation 'add/y' type=Const>,\n",
       " <tf.Operation 'add' type=AddV2>,\n",
       " <tf.Operation 'add_1/y' type=Const>,\n",
       " <tf.Operation 'add_1' type=AddV2>,\n",
       " <tf.Operation 'add_2/y' type=Const>,\n",
       " <tf.Operation 'add_2' type=AddV2>,\n",
       " <tf.Operation 'add_3/y' type=Const>,\n",
       " <tf.Operation 'add_3' type=AddV2>,\n",
       " <tf.Operation 'add_4/y' type=Const>,\n",
       " <tf.Operation 'add_4' type=AddV2>,\n",
       " <tf.Operation 'add_5/y' type=Const>,\n",
       " <tf.Operation 'add_5' type=AddV2>,\n",
       " <tf.Operation 'add_6/y' type=Const>,\n",
       " <tf.Operation 'add_6' type=AddV2>,\n",
       " <tf.Operation 'add_7/y' type=Const>,\n",
       " <tf.Operation 'add_7' type=AddV2>,\n",
       " <tf.Operation 'add_8/y' type=Const>,\n",
       " <tf.Operation 'add_8' type=AddV2>,\n",
       " <tf.Operation 'add_9/y' type=Const>,\n",
       " <tf.Operation 'add_9' type=AddV2>,\n",
       " <tf.Operation 'Identity' type=Identity>]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_10.get_concrete_function(tf.constant(5)).graph.get_operations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A \"dynamic\" loop using `tf.while_loop()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – shows how to use tf.while_loop (usually @tf.function is simpler)\n",
    "@tf.function\n",
    "def add_10(x):\n",
    "    condition = lambda i, x: tf.less(i, 10)\n",
    "    body = lambda i, x: (tf.add(i, 1), tf.add(x, 1))\n",
    "    final_i, final_x = tf.while_loop(condition, body, [tf.constant(0), x])\n",
    "    return final_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=15>"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_10(tf.constant(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'x' type=Placeholder>,\n",
       " <tf.Operation 'Const' type=Const>,\n",
       " <tf.Operation 'while/maximum_iterations' type=Const>,\n",
       " <tf.Operation 'while/loop_counter' type=Const>,\n",
       " <tf.Operation 'while' type=StatelessWhile>,\n",
       " <tf.Operation 'Identity' type=Identity>]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_10.get_concrete_function(tf.constant(5)).graph.get_operations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A \"dynamic\" `for` loop using `tf.range()` (captured by autograph):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def add_10(x):\n",
    "    for i in tf.range(10):\n",
    "        x = x + 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'x' type=Placeholder>,\n",
       " <tf.Operation 'range/start' type=Const>,\n",
       " <tf.Operation 'range/limit' type=Const>,\n",
       " <tf.Operation 'range/delta' type=Const>,\n",
       " <tf.Operation 'range' type=Range>,\n",
       " <tf.Operation 'sub' type=Sub>,\n",
       " <tf.Operation 'floordiv' type=FloorDiv>,\n",
       " <tf.Operation 'mod' type=FloorMod>,\n",
       " <tf.Operation 'zeros_like' type=Const>,\n",
       " <tf.Operation 'NotEqual' type=NotEqual>,\n",
       " <tf.Operation 'Cast' type=Cast>,\n",
       " <tf.Operation 'add' type=AddV2>,\n",
       " <tf.Operation 'zeros_like_1' type=Const>,\n",
       " <tf.Operation 'Maximum' type=Maximum>,\n",
       " <tf.Operation 'while/maximum_iterations' type=Const>,\n",
       " <tf.Operation 'while/loop_counter' type=Const>,\n",
       " <tf.Operation 'while' type=StatelessWhile>,\n",
       " <tf.Operation 'Identity' type=Identity>]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_10.get_concrete_function(tf.constant(0)).graph.get_operations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Variables and Other Resources in TF Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=2>"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = tf.Variable(0)\n",
    "\n",
    "@tf.function\n",
    "def increment(counter, c=1):\n",
    "    return counter.assign_add(c)\n",
    "\n",
    "increment(counter)  # counter is now equal to 1\n",
    "increment(counter)  # counter is now equal to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"counter\"\n",
       "type: DT_RESOURCE"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_def = increment.get_concrete_function(counter).function_def\n",
    "function_def.signature.input_arg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = tf.Variable(0)\n",
    "\n",
    "@tf.function\n",
    "def increment(c=1):\n",
    "    return counter.assign_add(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=2>"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "increment()\n",
    "increment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"assignaddvariableop_resource\"\n",
       "type: DT_RESOURCE"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_def = increment.get_concrete_function().function_def\n",
    "function_def.signature.input_arg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Counter:\n",
    "    def __init__(self):\n",
    "        self.counter = tf.Variable(0)\n",
    "\n",
    "    @tf.function\n",
    "    def increment(self, c=1):\n",
    "        return self.counter.assign_add(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=2>"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Counter()\n",
    "c.increment()\n",
    "c.increment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def tf__add(x):\n",
      "    with ag__.FunctionScope('add_10', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\n",
      "        do_return = False\n",
      "        retval_ = ag__.UndefinedReturnValue()\n",
      "\n",
      "        def get_state():\n",
      "            return (x,)\n",
      "\n",
      "        def set_state(vars_):\n",
      "            nonlocal x\n",
      "            (x,) = vars_\n",
      "\n",
      "        def loop_body(itr):\n",
      "            nonlocal x\n",
      "            i = itr\n",
      "            x = ag__.ld(x)\n",
      "            x += 1\n",
      "        i = ag__.Undefined('i')\n",
      "        ag__.for_stmt(ag__.converted_call(ag__.ld(tf).range, (10,), None, fscope), None, loop_body, get_state, set_state, ('x',), {'iterate_names': 'i'})\n",
      "        try:\n",
      "            do_return = True\n",
      "            retval_ = ag__.ld(x)\n",
      "        except:\n",
      "            do_return = False\n",
      "            raise\n",
      "        return fscope.ret(retval_, do_return)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def add_10(x):\n",
    "    for i in tf.range(10):\n",
    "        x += 1\n",
    "    return x\n",
    "\n",
    "print(tf.autograph.to_code(add_10.python_function))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – shows how to display the autograph code with syntax highlighting\n",
    "def display_tf_code(func):\n",
    "    from IPython.display import display, Markdown\n",
    "    if hasattr(func, \"python_function\"):\n",
    "        func = func.python_function\n",
    "    code = tf.autograph.to_code(func)\n",
    "    display(Markdown(f'```python\\n{code}\\n```'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def tf__add(x):\n",
       "    with ag__.FunctionScope('add_10', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\n",
       "        do_return = False\n",
       "        retval_ = ag__.UndefinedReturnValue()\n",
       "\n",
       "        def get_state():\n",
       "            return (x,)\n",
       "\n",
       "        def set_state(vars_):\n",
       "            nonlocal x\n",
       "            (x,) = vars_\n",
       "\n",
       "        def loop_body(itr):\n",
       "            nonlocal x\n",
       "            i = itr\n",
       "            x = ag__.ld(x)\n",
       "            x += 1\n",
       "        i = ag__.Undefined('i')\n",
       "        ag__.for_stmt(ag__.converted_call(ag__.ld(tf).range, (10,), None, fscope), None, loop_body, get_state, set_state, ('x',), {'iterate_names': 'i'})\n",
       "        try:\n",
       "            do_return = True\n",
       "            retval_ = ag__.ld(x)\n",
       "        except:\n",
       "            do_return = False\n",
       "            raise\n",
       "        return fscope.ret(retval_, do_return)\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_tf_code(add_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TF Functions with tf.keras (or Not)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, tf.keras will automatically convert your custom code into TF Functions, no need to use\n",
    "`tf.function()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss function\n",
    "def my_mse(y_true, y_pred):\n",
    "    print(\"Tracing loss my_mse()\")\n",
    "    return tf.reduce_mean(tf.square(y_pred - y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom metric function\n",
    "def my_mae(y_true, y_pred):\n",
    "    print(\"Tracing metric my_mae()\")\n",
    "    return tf.reduce_mean(tf.abs(y_pred - y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom layer\n",
    "class MyDense(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name='kernel', \n",
    "                                      shape=(input_shape[1], self.units),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        self.biases = self.add_weight(name='bias', \n",
    "                                      shape=(self.units,),\n",
    "                                      initializer='zeros',\n",
    "                                      trainable=True)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, X):\n",
    "        print(\"Tracing MyDense.call()\")\n",
    "        return self.activation(X @ self.kernel + self.biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom model\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = MyDense(30, activation=\"relu\")\n",
    "        self.hidden2 = MyDense(30, activation=\"relu\")\n",
    "        self.output_ = MyDense(1)\n",
    "\n",
    "    def call(self, input):\n",
    "        print(\"Tracing MyModel.call()\")\n",
    "        hidden1 = self.hidden1(input)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = tf.keras.layers.concatenate([input, hidden2])\n",
    "        output = self.output_(concat)\n",
    "        return output\n",
    "\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=my_mse, optimizer=\"nadam\", metrics=[my_mae])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "Tracing MyModel.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing loss my_mse()\n",
      "Tracing metric my_mae()\n",
      "Tracing MyModel.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing loss my_mse()\n",
      "Tracing metric my_mae()\n",
      "296/363 [=======================>......] - ETA: 0s - loss: 1.5172 - my_mae: 0.8562Tracing MyModel.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing loss my_mse()\n",
      "Tracing metric my_mae()\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.3255 - my_mae: 0.7900 - val_loss: 0.5569 - val_my_mae: 0.4819\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 792us/step - loss: 0.4419 - my_mae: 0.4767 - val_loss: 0.4664 - val_my_mae: 0.4576\n",
      "162/162 [==============================] - 0s 460us/step - loss: 0.4164 - my_mae: 0.4639\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4163525104522705, 0.4639028012752533]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can turn this off by creating the model with `dynamic=True` (or calling `super().__init__(dynamic=True, **kwargs)` in the model's constructor):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(dynamic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=my_mse, optimizer=\"nadam\", metrics=[my_mae])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the custom code will be called at each iteration. Let's fit, validate and evaluate with tiny datasets to avoid getting too much output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing MyModel.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing loss my_mse()\n",
      "Tracing metric my_mae()\n",
      "Tracing MyModel.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing loss my_mse()\n",
      "Tracing metric my_mae()\n",
      "Tracing MyModel.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing loss my_mse()\n",
      "Tracing metric my_mae()\n",
      "Tracing MyModel.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing loss my_mse()\n",
      "Tracing metric my_mae()\n",
      "Tracing MyModel.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing loss my_mse()\n",
      "Tracing metric my_mae()\n",
      "Tracing MyModel.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing loss my_mse()\n",
      "Tracing metric my_mae()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.507260322570801, 2.0566811561584473]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled[:64], y_train[:64], epochs=1,\n",
    "          validation_data=(X_valid_scaled[:64], y_valid[:64]), verbose=0)\n",
    "model.evaluate(X_test_scaled[:64], y_test[:64], verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can compile a model with `run_eagerly=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=my_mse, optimizer=\"nadam\", metrics=[my_mae], run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing MyModel.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing loss my_mse()\n",
      "Tracing metric my_mae()\n",
      "Tracing MyModel.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing loss my_mse()\n",
      "Tracing metric my_mae()\n",
      "Tracing MyModel.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing loss my_mse()\n",
      "Tracing metric my_mae()\n",
      "Tracing MyModel.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing loss my_mse()\n",
      "Tracing metric my_mae()\n",
      "Tracing MyModel.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing loss my_mse()\n",
      "Tracing metric my_mae()\n",
      "Tracing MyModel.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing loss my_mse()\n",
      "Tracing metric my_mae()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.507260322570801, 2.0566811561584473]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled[:64], y_train[:64], epochs=1,\n",
    "          validation_data=(X_valid_scaled[:64], y_valid[:64]), verbose=0)\n",
    "model.evaluate(X_test_scaled[:64], y_test[:64], verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Material – Custom Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining custom optimizers is not very common, but in case you are one of the happy few who gets to write one, here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMomentumOptimizer(tf.keras.optimizers.Optimizer):\n",
    "    def __init__(self, learning_rate=0.001, momentum=0.9, name=\"MyMomentumOptimizer\", **kwargs):\n",
    "        \"\"\"Call super().__init__() and use _set_hyper() to store hyperparameters\"\"\"\n",
    "        super().__init__(name, **kwargs)\n",
    "        self._set_hyper(\"learning_rate\", kwargs.get(\"lr\", learning_rate)) # handle lr=learning_rate\n",
    "        self._set_hyper(\"decay\", self._initial_decay) # \n",
    "        self._set_hyper(\"momentum\", momentum)\n",
    "    \n",
    "    def _create_slots(self, var_list):\n",
    "        \"\"\"For each model variable, create the optimizer variable associated with it.\n",
    "        TensorFlow calls these optimizer variables \"slots\".\n",
    "        For momentum optimization, we need one momentum slot per model variable.\n",
    "        \"\"\"\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, \"momentum\")\n",
    "\n",
    "    @tf.function\n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "        \"\"\"Update the slots and perform one optimization step for one model variable\n",
    "        \"\"\"\n",
    "        var_dtype = var.dtype.base_dtype\n",
    "        lr_t = self._decayed_lr(var_dtype) # handle learning rate decay\n",
    "        momentum_var = self.get_slot(var, \"momentum\")\n",
    "        momentum_hyper = self._get_hyper(\"momentum\", var_dtype)\n",
    "        momentum_var.assign(momentum_var * momentum_hyper - (1. - momentum_hyper)* grad)\n",
    "        var.assign_add(momentum_var * lr_t)\n",
    "\n",
    "    def _resource_apply_sparse(self, grad, var):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {\n",
    "            **base_config,\n",
    "            \"learning_rate\": self._serialize_hyperparameter(\"learning_rate\"),\n",
    "            \"decay\": self._serialize_hyperparameter(\"decay\"),\n",
    "            \"momentum\": self._serialize_hyperparameter(\"momentum\"),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "363/363 [==============================] - 0s 444us/step - loss: 4.9648\n",
      "Epoch 2/5\n",
      "363/363 [==============================] - 0s 444us/step - loss: 1.7888\n",
      "Epoch 3/5\n",
      "363/363 [==============================] - 0s 437us/step - loss: 1.0021\n",
      "Epoch 4/5\n",
      "363/363 [==============================] - 0s 451us/step - loss: 0.7869\n",
      "Epoch 5/5\n",
      "363/363 [==============================] - 0s 446us/step - loss: 0.7122\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbfe0a94d50>"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=[8])])\n",
    "model.compile(loss=\"mse\", optimizer=MyMomentumOptimizer())\n",
    "model.fit(X_train_scaled, y_train, epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}