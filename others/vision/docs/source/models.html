
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Models and pre-trained weights &#8212; My sample book</title>
    
  <link href="../../../../_static/css/theme.css" rel="stylesheet">
  <link href="../../../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/togglebutton.js"></script>
    <script src="../../../../_static/clipboard.min.js"></script>
    <script src="../../../../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">My sample book</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../intro.html">
   Welcome to your Jupyter Book
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Cheat Sheet
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../pytorch/pytorch_cheatsheet.html">
   1. Pytorch Cheatsheet
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tensorflow basics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../old/1.tensor.html">
   2. Introduction to Tensors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../old/2.variable.html">
   3. Introduction to Variables
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../old/3.autodiff.html">
   4. Introduction to gradients and automatic differentiation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../old/5.intro_to_modules.html">
   5. Introduction to modules, layers, and models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../old/6.basic_training_loops.html">
   7. Basic training loops
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../old/7.keras_sequential_model.html">
   8. The Sequential model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../tf_create_model.html">
   9. 三種搭建神經網路的方式
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../hands_on_ml3/tf_customization.html">
   10. Customization
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Pytorch resource summarise
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../pytorch/d2l/d2l_linear_regression.html">
   11. Linear regression (d2l)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../pytorch/d2l/d2l_softmax_regression.html">
   12. Softmax regression (d2l)
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../../../_sources/others/vision/docs/source/models.rst"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.rst</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/executablebooks/jupyter-book"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fothers/vision/docs/source/models.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#general-information-on-pre-trained-weights">
   General information on pre-trained weights
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#initializing-pre-trained-models">
     Initializing pre-trained models
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-the-pre-trained-models">
     Using the pre-trained models
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-registration-mechanism">
     Model Registration Mechanism
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-models-from-hub">
     Using models from Hub
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#classification">
   Classification
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#table-of-all-available-classification-weights">
     Table of all available classification weights
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#quantized-models">
     Quantized models
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#table-of-all-available-quantized-classification-weights">
       Table of all available quantized classification weights
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#semantic-segmentation">
   Semantic Segmentation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#table-of-all-available-semantic-segmentation-weights">
     Table of all available semantic segmentation weights
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#object-detection-instance-segmentation-and-person-keypoint-detection">
   Object Detection, Instance Segmentation and Person Keypoint Detection
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#object-detection">
     Object Detection
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#table-of-all-available-object-detection-weights">
       Table of all available Object detection weights
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#instance-segmentation">
     Instance Segmentation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#table-of-all-available-instance-segmentation-weights">
       Table of all available Instance segmentation weights
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#keypoint-detection">
     Keypoint Detection
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#table-of-all-available-keypoint-detection-weights">
       Table of all available Keypoint detection weights
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#video-classification">
   Video Classification
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#table-of-all-available-video-classification-weights">
     Table of all available video classification weights
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optical-flow">
   Optical Flow
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Models and pre-trained weights</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#general-information-on-pre-trained-weights">
   General information on pre-trained weights
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#initializing-pre-trained-models">
     Initializing pre-trained models
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-the-pre-trained-models">
     Using the pre-trained models
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-registration-mechanism">
     Model Registration Mechanism
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-models-from-hub">
     Using models from Hub
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#classification">
   Classification
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#table-of-all-available-classification-weights">
     Table of all available classification weights
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#quantized-models">
     Quantized models
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#table-of-all-available-quantized-classification-weights">
       Table of all available quantized classification weights
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#semantic-segmentation">
   Semantic Segmentation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#table-of-all-available-semantic-segmentation-weights">
     Table of all available semantic segmentation weights
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#object-detection-instance-segmentation-and-person-keypoint-detection">
   Object Detection, Instance Segmentation and Person Keypoint Detection
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#object-detection">
     Object Detection
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#table-of-all-available-object-detection-weights">
       Table of all available Object detection weights
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#instance-segmentation">
     Instance Segmentation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#table-of-all-available-instance-segmentation-weights">
       Table of all available Instance segmentation weights
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#keypoint-detection">
     Keypoint Detection
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#table-of-all-available-keypoint-detection-weights">
       Table of all available Keypoint detection weights
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#video-classification">
   Video Classification
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#table-of-all-available-video-classification-weights">
     Table of all available video classification weights
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optical-flow">
   Optical Flow
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="section" id="models-and-pre-trained-weights">
<span id="models"></span><h1>Models and pre-trained weights<a class="headerlink" href="#models-and-pre-trained-weights" title="Permalink to this headline">¶</a></h1>
<p>The <code class="docutils literal notranslate"><span class="pre">torchvision.models</span></code> subpackage contains definitions of models for addressing
different tasks, including: image classification, pixelwise semantic
segmentation, object detection, instance segmentation, person
keypoint detection, video classification, and optical flow.</p>
<div class="section" id="general-information-on-pre-trained-weights">
<h2>General information on pre-trained weights<a class="headerlink" href="#general-information-on-pre-trained-weights" title="Permalink to this headline">¶</a></h2>
<p>TorchVision offers pre-trained weights for every provided architecture, using
the PyTorch <code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.hub</span></code>. Instancing a pre-trained model will download its
weights to a cache directory. This directory can be set using the <cite>TORCH_HOME</cite>
environment variable. See <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.hub.load_state_dict_from_url()</span></code> for details.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The pre-trained models provided in this library may have their own licenses or
terms and conditions derived from the dataset used for training. It is your
responsibility to determine whether you have permission to use the models for
your use case.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Backward compatibility is guaranteed for loading a serialized
<code class="docutils literal notranslate"><span class="pre">state_dict</span></code> to the model created using old PyTorch version.
On the contrary, loading entire saved models or serialized
<code class="docutils literal notranslate"><span class="pre">ScriptModules</span></code> (serialized using older versions of PyTorch)
may not preserve the historic behaviour. Refer to the following
<a class="reference external" href="https://pytorch.org/docs/stable/notes/serialization.html#id6">documentation</a></p>
</div>
<div class="section" id="initializing-pre-trained-models">
<h3>Initializing pre-trained models<a class="headerlink" href="#initializing-pre-trained-models" title="Permalink to this headline">¶</a></h3>
<p>As of v0.13, TorchVision offers a new <a class="reference external" href="https://pytorch.org/blog/introducing-torchvision-new-multi-weight-support-api/">Multi-weight support API</a>
for loading different weights to the existing model builder methods:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchvision.models</span> <span class="kn">import</span> <span class="n">resnet50</span><span class="p">,</span> <span class="n">ResNet50_Weights</span>

<span class="c1"># Old weights with accuracy 76.130%</span>
<span class="n">resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">ResNet50_Weights</span><span class="o">.</span><span class="n">IMAGENET1K_V1</span><span class="p">)</span>

<span class="c1"># New weights with accuracy 80.858%</span>
<span class="n">resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">ResNet50_Weights</span><span class="o">.</span><span class="n">IMAGENET1K_V2</span><span class="p">)</span>

<span class="c1"># Best available weights (currently alias for IMAGENET1K_V2)</span>
<span class="c1"># Note that these weights may change across versions</span>
<span class="n">resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">ResNet50_Weights</span><span class="o">.</span><span class="n">DEFAULT</span><span class="p">)</span>

<span class="c1"># Strings are also supported</span>
<span class="n">resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="s2">&quot;IMAGENET1K_V2&quot;</span><span class="p">)</span>

<span class="c1"># No weights - random initialization</span>
<span class="n">resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
<p>Migrating to the new API is very straightforward. The following method calls between the 2 APIs are all equivalent:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchvision.models</span> <span class="kn">import</span> <span class="n">resnet50</span><span class="p">,</span> <span class="n">ResNet50_Weights</span>

<span class="c1"># Using pretrained weights:</span>
<span class="n">resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">ResNet50_Weights</span><span class="o">.</span><span class="n">IMAGENET1K_V1</span><span class="p">)</span>
<span class="n">resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="s2">&quot;IMAGENET1K_V1&quot;</span><span class="p">)</span>
<span class="n">resnet50</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># deprecated</span>
<span class="n">resnet50</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># deprecated</span>

<span class="c1"># Using no weights:</span>
<span class="n">resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">resnet50</span><span class="p">()</span>
<span class="n">resnet50</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># deprecated</span>
<span class="n">resnet50</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># deprecated</span>
</pre></div>
</div>
<p>Note that the <code class="docutils literal notranslate"><span class="pre">pretrained</span></code> parameter is now deprecated, using it will emit warnings and will be removed on v0.15.</p>
</div>
<div class="section" id="using-the-pre-trained-models">
<h3>Using the pre-trained models<a class="headerlink" href="#using-the-pre-trained-models" title="Permalink to this headline">¶</a></h3>
<p>Before using the pre-trained models, one must preprocess the image
(resize with right resolution/interpolation, apply inference transforms,
rescale the values etc). There is no standard way to do this as it depends on
how a given model was trained. It can vary across model families, variants or
even weight versions. Using the correct preprocessing method is critical and
failing to do so may lead to decreased accuracy or incorrect outputs.</p>
<p>All the necessary information for the inference transforms of each pre-trained
model is provided on its weights documentation. To simplify inference, TorchVision
bundles the necessary preprocessing transforms into each model weight. These are
accessible via the <code class="docutils literal notranslate"><span class="pre">weight.transforms</span></code> attribute:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialize the Weight Transforms</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">ResNet50_Weights</span><span class="o">.</span><span class="n">DEFAULT</span>
<span class="n">preprocess</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">transforms</span><span class="p">()</span>

<span class="c1"># Apply it to the input image</span>
<span class="n">img_transformed</span> <span class="o">=</span> <span class="n">preprocess</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
</pre></div>
</div>
<p>Some models use modules which have different training and evaluation
behavior, such as batch normalization. To switch between these modes, use
<code class="docutils literal notranslate"><span class="pre">model.train()</span></code> or <code class="docutils literal notranslate"><span class="pre">model.eval()</span></code> as appropriate. See
<code class="xref py py-meth docutils literal notranslate"><span class="pre">train()</span></code> or <code class="xref py py-meth docutils literal notranslate"><span class="pre">eval()</span></code> for details.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialize model</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">ResNet50_Weights</span><span class="o">.</span><span class="n">DEFAULT</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>

<span class="c1"># Set model to eval mode</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="model-registration-mechanism">
<h3>Model Registration Mechanism<a class="headerlink" href="#model-registration-mechanism" title="Permalink to this headline">¶</a></h3>
<p>As of v0.14, TorchVision offers a new model registration mechanism which allows retreaving models
and weights by their names. Here are a few examples on how to use them:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># List available models</span>
<span class="n">all_models</span> <span class="o">=</span> <span class="n">list_models</span><span class="p">()</span>
<span class="n">classification_models</span> <span class="o">=</span> <span class="n">list_models</span><span class="p">(</span><span class="n">module</span><span class="o">=</span><span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="p">)</span>

<span class="c1"># Initialize models</span>
<span class="n">m1</span> <span class="o">=</span> <span class="n">get_model</span><span class="p">(</span><span class="s2">&quot;mobilenet_v3_large&quot;</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">m2</span> <span class="o">=</span> <span class="n">get_model</span><span class="p">(</span><span class="s2">&quot;quantized_mobilenet_v3_large&quot;</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="s2">&quot;DEFAULT&quot;</span><span class="p">)</span>

<span class="c1"># Fetch weights</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">get_weight</span><span class="p">(</span><span class="s2">&quot;MobileNet_V3_Large_QuantizedWeights.DEFAULT&quot;</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">weights</span> <span class="o">==</span> <span class="n">MobileNet_V3_Large_QuantizedWeights</span><span class="o">.</span><span class="n">DEFAULT</span>

<span class="n">weights_enum</span> <span class="o">=</span> <span class="n">get_model_weights</span><span class="p">(</span><span class="s2">&quot;quantized_mobilenet_v3_large&quot;</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">weights_enum</span> <span class="o">==</span> <span class="n">MobileNet_V3_Large_QuantizedWeights</span>

<span class="n">weights_enum2</span> <span class="o">=</span> <span class="n">get_model_weights</span><span class="p">(</span><span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">mobilenet_v3_large</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">weights_enum</span> <span class="o">==</span> <span class="n">weights_enum2</span>
</pre></div>
</div>
<p>Here are the available public methods of the model registration mechanism:</p>
</div>
<div class="section" id="using-models-from-hub">
<h3>Using models from Hub<a class="headerlink" href="#using-models-from-hub" title="Permalink to this headline">¶</a></h3>
<p>Most pre-trained models can be accessed directly via PyTorch Hub without having TorchVision installed:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="c1"># Option 1: passing weights param as string</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;pytorch/vision&quot;</span><span class="p">,</span> <span class="s2">&quot;resnet50&quot;</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="s2">&quot;IMAGENET1K_V2&quot;</span><span class="p">)</span>

<span class="c1"># Option 2: passing weights param as enum</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;pytorch/vision&quot;</span><span class="p">,</span> <span class="s2">&quot;get_weight&quot;</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="s2">&quot;ResNet50_Weights.IMAGENET1K_V2&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;pytorch/vision&quot;</span><span class="p">,</span> <span class="s2">&quot;resnet50&quot;</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>
</pre></div>
</div>
<p>You can also retrieve all the available weights of a specific model via PyTorch Hub by doing:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">weight_enum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;pytorch/vision&quot;</span><span class="p">,</span> <span class="s2">&quot;get_model_weights&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;resnet50&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">([</span><span class="n">weight</span> <span class="k">for</span> <span class="n">weight</span> <span class="ow">in</span> <span class="n">weight_enum</span><span class="p">])</span>
</pre></div>
</div>
<p>The only exception to the above are the detection models included on
<code class="xref py py-mod docutils literal notranslate"><span class="pre">torchvision.models.detection</span></code>. These models require TorchVision
to be installed because they depend on custom C++ operators.</p>
</div>
</div>
<div class="section" id="classification">
<h2>Classification<a class="headerlink" href="#classification" title="Permalink to this headline">¶</a></h2>
<p>The following classification models are available, with or without pre-trained
weights:</p>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="models/alexnet.html">AlexNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/convnext.html">ConvNeXt</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/densenet.html">DenseNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/efficientnet.html">EfficientNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/efficientnetv2.html">EfficientNetV2</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/googlenet.html">GoogLeNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/inception.html">Inception V3</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/maxvit.html">MaxVit</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/mnasnet.html">MNASNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/mobilenetv2.html">MobileNet V2</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/mobilenetv3.html">MobileNet V3</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/regnet.html">RegNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/resnet.html">ResNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/resnext.html">ResNeXt</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/shufflenetv2.html">ShuffleNet V2</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/squeezenet.html">SqueezeNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/swin_transformer.html">SwinTransformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/vgg.html">VGG</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/vision_transformer.html">VisionTransformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/wide_resnet.html">Wide ResNet</a></li>
</ul>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>Here is an example of how to use the pre-trained image classification models:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchvision.io</span> <span class="kn">import</span> <span class="n">read_image</span>
<span class="kn">from</span> <span class="nn">torchvision.models</span> <span class="kn">import</span> <span class="n">resnet50</span><span class="p">,</span> <span class="n">ResNet50_Weights</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">read_image</span><span class="p">(</span><span class="s2">&quot;test/assets/encode_jpeg/grace_hopper_517x606.jpg&quot;</span><span class="p">)</span>

<span class="c1"># Step 1: Initialize model with the best available weights</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">ResNet50_Weights</span><span class="o">.</span><span class="n">DEFAULT</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># Step 2: Initialize the inference transforms</span>
<span class="n">preprocess</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">transforms</span><span class="p">()</span>

<span class="c1"># Step 3: Apply inference preprocessing transforms</span>
<span class="n">batch</span> <span class="o">=</span> <span class="n">preprocess</span><span class="p">(</span><span class="n">img</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Step 4: Use the model and print the predicted category</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">class_id</span> <span class="o">=</span> <span class="n">prediction</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="n">score</span> <span class="o">=</span> <span class="n">prediction</span><span class="p">[</span><span class="n">class_id</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="n">category_name</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;categories&quot;</span><span class="p">][</span><span class="n">class_id</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">category_name</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="mi">100</span> <span class="o">*</span> <span class="n">score</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The classes of the pre-trained model outputs can be found at <code class="docutils literal notranslate"><span class="pre">weights.meta[&quot;categories&quot;]</span></code>.</p>
<div class="section" id="table-of-all-available-classification-weights">
<h3>Table of all available classification weights<a class="headerlink" href="#table-of-all-available-classification-weights" title="Permalink to this headline">¶</a></h3>
<p>Accuracies are reported on ImageNet-1K using single crops:</p>
</div>
<div class="section" id="quantized-models">
<h3>Quantized models<a class="headerlink" href="#quantized-models" title="Permalink to this headline">¶</a></h3>
<p>The following architectures provide support for INT8 quantized models, with or without
pre-trained weights:</p>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="models/googlenet_quant.html">Quantized GoogLeNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/inception_quant.html">Quantized InceptionV3</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/mobilenetv2_quant.html">Quantized MobileNet V2</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/mobilenetv3_quant.html">Quantized MobileNet V3</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/resnet_quant.html">Quantized ResNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/resnext_quant.html">Quantized ResNeXt</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/shufflenetv2_quant.html">Quantized ShuffleNet V2</a></li>
</ul>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>Here is an example of how to use the pre-trained quantized image classification models:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchvision.io</span> <span class="kn">import</span> <span class="n">read_image</span>
<span class="kn">from</span> <span class="nn">torchvision.models.quantization</span> <span class="kn">import</span> <span class="n">resnet50</span><span class="p">,</span> <span class="n">ResNet50_QuantizedWeights</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">read_image</span><span class="p">(</span><span class="s2">&quot;test/assets/encode_jpeg/grace_hopper_517x606.jpg&quot;</span><span class="p">)</span>

<span class="c1"># Step 1: Initialize model with the best available weights</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">ResNet50_QuantizedWeights</span><span class="o">.</span><span class="n">DEFAULT</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">,</span> <span class="n">quantize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># Step 2: Initialize the inference transforms</span>
<span class="n">preprocess</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">transforms</span><span class="p">()</span>

<span class="c1"># Step 3: Apply inference preprocessing transforms</span>
<span class="n">batch</span> <span class="o">=</span> <span class="n">preprocess</span><span class="p">(</span><span class="n">img</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Step 4: Use the model and print the predicted category</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">class_id</span> <span class="o">=</span> <span class="n">prediction</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="n">score</span> <span class="o">=</span> <span class="n">prediction</span><span class="p">[</span><span class="n">class_id</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="n">category_name</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;categories&quot;</span><span class="p">][</span><span class="n">class_id</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">category_name</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="mi">100</span> <span class="o">*</span> <span class="n">score</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The classes of the pre-trained model outputs can be found at <code class="docutils literal notranslate"><span class="pre">weights.meta[&quot;categories&quot;]</span></code>.</p>
<div class="section" id="table-of-all-available-quantized-classification-weights">
<h4>Table of all available quantized classification weights<a class="headerlink" href="#table-of-all-available-quantized-classification-weights" title="Permalink to this headline">¶</a></h4>
<p>Accuracies are reported on ImageNet-1K using single crops:</p>
</div>
</div>
</div>
<div class="section" id="semantic-segmentation">
<h2>Semantic Segmentation<a class="headerlink" href="#semantic-segmentation" title="Permalink to this headline">¶</a></h2>
<p>The following semantic segmentation models are available, with or without
pre-trained weights:</p>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="models/deeplabv3.html">DeepLabV3</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/fcn.html">FCN</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/lraspp.html">LRASPP</a></li>
</ul>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>Here is an example of how to use the pre-trained semantic segmentation models:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchvision.io.image</span> <span class="kn">import</span> <span class="n">read_image</span>
<span class="kn">from</span> <span class="nn">torchvision.models.segmentation</span> <span class="kn">import</span> <span class="n">fcn_resnet50</span><span class="p">,</span> <span class="n">FCN_ResNet50_Weights</span>
<span class="kn">from</span> <span class="nn">torchvision.transforms.functional</span> <span class="kn">import</span> <span class="n">to_pil_image</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">read_image</span><span class="p">(</span><span class="s2">&quot;gallery/assets/dog1.jpg&quot;</span><span class="p">)</span>

<span class="c1"># Step 1: Initialize model with the best available weights</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">FCN_ResNet50_Weights</span><span class="o">.</span><span class="n">DEFAULT</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">fcn_resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># Step 2: Initialize the inference transforms</span>
<span class="n">preprocess</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">transforms</span><span class="p">()</span>

<span class="c1"># Step 3: Apply inference preprocessing transforms</span>
<span class="n">batch</span> <span class="o">=</span> <span class="n">preprocess</span><span class="p">(</span><span class="n">img</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Step 4: Use the model and visualize the prediction</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)[</span><span class="s2">&quot;out&quot;</span><span class="p">]</span>
<span class="n">normalized_masks</span> <span class="o">=</span> <span class="n">prediction</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">class_to_idx</span> <span class="o">=</span> <span class="p">{</span><span class="bp">cls</span><span class="p">:</span> <span class="n">idx</span> <span class="k">for</span> <span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">weights</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;categories&quot;</span><span class="p">])}</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">normalized_masks</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">class_to_idx</span><span class="p">[</span><span class="s2">&quot;dog&quot;</span><span class="p">]]</span>
<span class="n">to_pil_image</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>The classes of the pre-trained model outputs can be found at <code class="docutils literal notranslate"><span class="pre">weights.meta[&quot;categories&quot;]</span></code>.
The output format of the models is illustrated in <span class="xref std std-ref">semantic_seg_output</span>.</p>
<div class="section" id="table-of-all-available-semantic-segmentation-weights">
<h3>Table of all available semantic segmentation weights<a class="headerlink" href="#table-of-all-available-semantic-segmentation-weights" title="Permalink to this headline">¶</a></h3>
<p>All models are evaluated a subset of COCO val2017, on the 20 categories that are present in the Pascal VOC dataset:</p>
</div>
</div>
<div class="section" id="object-detection-instance-segmentation-and-person-keypoint-detection">
<span id="object-det-inst-seg-pers-keypoint-det"></span><h2>Object Detection, Instance Segmentation and Person Keypoint Detection<a class="headerlink" href="#object-detection-instance-segmentation-and-person-keypoint-detection" title="Permalink to this headline">¶</a></h2>
<p>The pre-trained models for detection, instance segmentation and
keypoint detection are initialized with the classification models
in torchvision. The models expect a list of <code class="docutils literal notranslate"><span class="pre">Tensor[C,</span> <span class="pre">H,</span> <span class="pre">W]</span></code>.
Check the constructor of the models for more information.</p>
<div class="section" id="object-detection">
<h3>Object Detection<a class="headerlink" href="#object-detection" title="Permalink to this headline">¶</a></h3>
<p>The following object detection models are available, with or without pre-trained
weights:</p>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="models/faster_rcnn.html">Faster R-CNN</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/fcos.html">FCOS</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/retinanet.html">RetinaNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/ssd.html">SSD</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/ssdlite.html">SSDlite</a></li>
</ul>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>Here is an example of how to use the pre-trained object detection models:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchvision.io.image</span> <span class="kn">import</span> <span class="n">read_image</span>
<span class="kn">from</span> <span class="nn">torchvision.models.detection</span> <span class="kn">import</span> <span class="n">fasterrcnn_resnet50_fpn_v2</span><span class="p">,</span> <span class="n">FasterRCNN_ResNet50_FPN_V2_Weights</span>
<span class="kn">from</span> <span class="nn">torchvision.utils</span> <span class="kn">import</span> <span class="n">draw_bounding_boxes</span>
<span class="kn">from</span> <span class="nn">torchvision.transforms.functional</span> <span class="kn">import</span> <span class="n">to_pil_image</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">read_image</span><span class="p">(</span><span class="s2">&quot;test/assets/encode_jpeg/grace_hopper_517x606.jpg&quot;</span><span class="p">)</span>

<span class="c1"># Step 1: Initialize model with the best available weights</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">FasterRCNN_ResNet50_FPN_V2_Weights</span><span class="o">.</span><span class="n">DEFAULT</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">fasterrcnn_resnet50_fpn_v2</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">,</span> <span class="n">box_score_thresh</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># Step 2: Initialize the inference transforms</span>
<span class="n">preprocess</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">transforms</span><span class="p">()</span>

<span class="c1"># Step 3: Apply inference preprocessing transforms</span>
<span class="n">batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">preprocess</span><span class="p">(</span><span class="n">img</span><span class="p">)]</span>

<span class="c1"># Step 4: Use the model and visualize the prediction</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">weights</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;categories&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">prediction</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">]]</span>
<span class="n">box</span> <span class="o">=</span> <span class="n">draw_bounding_boxes</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">boxes</span><span class="o">=</span><span class="n">prediction</span><span class="p">[</span><span class="s2">&quot;boxes&quot;</span><span class="p">],</span>
                          <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
                          <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span>
                          <span class="n">width</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">font_size</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">im</span> <span class="o">=</span> <span class="n">to_pil_image</span><span class="p">(</span><span class="n">box</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
<span class="n">im</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>The classes of the pre-trained model outputs can be found at <code class="docutils literal notranslate"><span class="pre">weights.meta[&quot;categories&quot;]</span></code>.
For details on how to plot the bounding boxes of the models, you may refer to <span class="xref std std-ref">instance_seg_output</span>.</p>
<div class="section" id="table-of-all-available-object-detection-weights">
<h4>Table of all available Object detection weights<a class="headerlink" href="#table-of-all-available-object-detection-weights" title="Permalink to this headline">¶</a></h4>
<p>Box MAPs are reported on COCO val2017:</p>
</div>
</div>
<div class="section" id="instance-segmentation">
<h3>Instance Segmentation<a class="headerlink" href="#instance-segmentation" title="Permalink to this headline">¶</a></h3>
<p>The following instance segmentation models are available, with or without pre-trained
weights:</p>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="models/mask_rcnn.html">Mask R-CNN</a></li>
</ul>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>For details on how to plot the masks of the models, you may refer to <span class="xref std std-ref">instance_seg_output</span>.</p>
<div class="section" id="table-of-all-available-instance-segmentation-weights">
<h4>Table of all available Instance segmentation weights<a class="headerlink" href="#table-of-all-available-instance-segmentation-weights" title="Permalink to this headline">¶</a></h4>
<p>Box and Mask MAPs are reported on COCO val2017:</p>
</div>
</div>
<div class="section" id="keypoint-detection">
<h3>Keypoint Detection<a class="headerlink" href="#keypoint-detection" title="Permalink to this headline">¶</a></h3>
<p>The following person keypoint detection models are available, with or without
pre-trained weights:</p>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="models/keypoint_rcnn.html">Keypoint R-CNN</a></li>
</ul>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>The classes of the pre-trained model outputs can be found at <code class="docutils literal notranslate"><span class="pre">weights.meta[&quot;keypoint_names&quot;]</span></code>.
For details on how to plot the bounding boxes of the models, you may refer to <span class="xref std std-ref">keypoint_output</span>.</p>
<div class="section" id="table-of-all-available-keypoint-detection-weights">
<h4>Table of all available Keypoint detection weights<a class="headerlink" href="#table-of-all-available-keypoint-detection-weights" title="Permalink to this headline">¶</a></h4>
<p>Box and Keypoint MAPs are reported on COCO val2017:</p>
</div>
</div>
</div>
<div class="section" id="video-classification">
<h2>Video Classification<a class="headerlink" href="#video-classification" title="Permalink to this headline">¶</a></h2>
<p>The following video classification models are available, with or without
pre-trained weights:</p>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="models/video_mvit.html">Video MViT</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/video_resnet.html">Video ResNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/video_s3d.html">Video S3D</a></li>
<li class="toctree-l1"><a class="reference internal" href="models/video_swin_transformer.html">Video SwinTransformer</a></li>
</ul>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>Here is an example of how to use the pre-trained video classification models:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchvision.io.video</span> <span class="kn">import</span> <span class="n">read_video</span>
<span class="kn">from</span> <span class="nn">torchvision.models.video</span> <span class="kn">import</span> <span class="n">r3d_18</span><span class="p">,</span> <span class="n">R3D_18_Weights</span>

<span class="n">vid</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">read_video</span><span class="p">(</span><span class="s2">&quot;test/assets/videos/v_SoccerJuggling_g23_c01.avi&quot;</span><span class="p">,</span> <span class="n">output_format</span><span class="o">=</span><span class="s2">&quot;TCHW&quot;</span><span class="p">)</span>
<span class="n">vid</span> <span class="o">=</span> <span class="n">vid</span><span class="p">[:</span><span class="mi">32</span><span class="p">]</span>  <span class="c1"># optionally shorten duration</span>

<span class="c1"># Step 1: Initialize model with the best available weights</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">R3D_18_Weights</span><span class="o">.</span><span class="n">DEFAULT</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">r3d_18</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># Step 2: Initialize the inference transforms</span>
<span class="n">preprocess</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">transforms</span><span class="p">()</span>

<span class="c1"># Step 3: Apply inference preprocessing transforms</span>
<span class="n">batch</span> <span class="o">=</span> <span class="n">preprocess</span><span class="p">(</span><span class="n">vid</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Step 4: Use the model and print the predicted category</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">label</span> <span class="o">=</span> <span class="n">prediction</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="n">score</span> <span class="o">=</span> <span class="n">prediction</span><span class="p">[</span><span class="n">label</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="n">category_name</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;categories&quot;</span><span class="p">][</span><span class="n">label</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">category_name</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="mi">100</span> <span class="o">*</span> <span class="n">score</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The classes of the pre-trained model outputs can be found at <code class="docutils literal notranslate"><span class="pre">weights.meta[&quot;categories&quot;]</span></code>.</p>
<div class="section" id="table-of-all-available-video-classification-weights">
<h3>Table of all available video classification weights<a class="headerlink" href="#table-of-all-available-video-classification-weights" title="Permalink to this headline">¶</a></h3>
<p>Accuracies are reported on Kinetics-400 using single crops for clip length 16:</p>
</div>
</div>
<div class="section" id="optical-flow">
<h2>Optical Flow<a class="headerlink" href="#optical-flow" title="Permalink to this headline">¶</a></h2>
<p>The following Optical Flow models are available, with or without pre-trained</p>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="models/raft.html">RAFT</a></li>
</ul>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./others/vision/docs/source"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By The Jupyter Book Community<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../../../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>