{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to modules, layers, and models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0DdlfacAdTZ"
   },
   "source": [
    "* 在用 tensorflow 做 machine learning的時候，會需要去 define, save, and restore a model.\n",
    "* 在 tf 中， model 可被定義為： \n",
    "  * A function that computes something on tensors (a **forward pass**)  \n",
    "  * Some variables that can be updated in response to training  \n",
    "* 在這份文件中，將 go below the surface of Keras to see how TensorFlow models are defined. \n",
    "* This looks at how TensorFlow collects variables and models, as well as how they are saved and restored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSa6ayJmfZxZ"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "goZwOXp_xyQj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yt5HEbsYAbw1"
   },
   "source": [
    "## `tf.Module`: Defining models and layers in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 大部分的 model 都是由 layers 所組成. \n",
    "* Layers 其實就是 functions，而這個 function 是由可被重複使用的數學結構所定義，裡面有可訓練的變數。  \n",
    "* 在 tensorflow 中，大部分 high-level 的 layers 和 models，都是built on the same foundational class: `tf.Module`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "alhYPVEtAiSy",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=30.0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SimpleModule(tf.Module):\n",
    "  def __init__(self, name=None):\n",
    "    super().__init__(name=name)\n",
    "    self.a_variable = tf.Variable(5.0, name=\"train_me\")\n",
    "    self.non_trainable_variable = tf.Variable(5.0, trainable=False, name=\"do_not_train_me\")\n",
    "  def __call__(self, x):\n",
    "    return self.a_variable * x + self.non_trainable_variable\n",
    "\n",
    "simple_module = SimpleModule(name=\"simple\")\n",
    "\n",
    "simple_module(tf.constant(5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwMc_zu5Ant8"
   },
   "source": [
    "* 來看一下，我們剛剛定義了一個 module (你可以叫他 module，也可以叫他 layer，都可以)，他最一開始就繼承了 `tf.Moudle` 這個 class\n",
    "* 可以看到，這個 class，有 `__call__` 這個 method，這就是一般 python callable 的定義方式，沒什麼特別\n",
    "* 再來看一下 `__init__` 裡面，定義了兩個會用的屬性，也就是兩個 `tf.Variable`。其中一個是 trainable (可微分)，另一個是不給 train (不可微分)\n",
    "* 然後，這個 module 要做的事情，就是，當你輸入 x 時，他會幫你乘上一個數，然後再加上一個數\n",
    "* 從結果來看，輸入 5 後，得到 32\n",
    "* 那，繼承 `tf.Module` 這個 class 有什麼好處？好處就是，已經有寫好一些 method 和 attribute，會幫你省很多力氣。例如，他會自動幫你蒐集總共定義了多少個 tf.Variables，以及， trainable 的 variable 有哪些，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'train_me:0' shape=() dtype=float32, numpy=5.0>,\n",
       " <tf.Variable 'do_not_train_me:0' shape=() dtype=float32, numpy=5.0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all variables\n",
    "simple_module.variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 可以看到，他用一個 tuple，把所有 variable 給蒐集起來。名稱分別是 `train_me` 和 `do_not_train_me`. \n",
    "* 另外，自動微分最常用的，就是抓出 trainable variables 來做微分。這他也幫你蒐集好了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'train_me:0' shape=() dtype=float32, numpy=5.0>,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainable variable\n",
    "simple_module.trainable_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自訂 layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 接著，我們可以來定義一個自己的 `Dense` (linear) layer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Efb2p2bzAn-V",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dense(tf.Module):\n",
    "  def __init__(self, in_features, out_features, name=None):\n",
    "    super().__init__(name=name)\n",
    "    # Dense layer 的第一個參數，是 weight 矩陣 `w`，起始值用 normal 來生，shape 是 in_features x out_features\n",
    "    self.w = tf.Variable(\n",
    "      tf.random.normal([in_features, out_features]), name='w')\n",
    "    # Dense layer 的第二個參數，是 bias 向量 `b`，起始值給 0 向量， shape 為 1 x out_features\n",
    "    self.b = tf.Variable(tf.zeros([out_features]), name='b')\n",
    "  def __call__(self, x):\n",
    "    # 當我們 call Dense 的時候，就是輸入一個 x tensor (shape 為 1 x in_features)，然後去計算 x w + b，得到 shape 為 1 x out_features 的向量 \n",
    "    y = tf.matmul(x, self.w) + self.b\n",
    "    return tf.nn.relu(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 可以想像， w 和 b 都是 trainable variable，之後就要靠自動微分來更新 w 和 b 的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'b:0' shape=(6,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'w:0' shape=(3, 6) dtype=float32, numpy=\n",
       " array([[-0.2585963 , -0.72111183, -0.47151518,  1.1848954 ,  1.4632958 ,\n",
       "          1.9215399 ],\n",
       "        [ 1.2155559 , -0.9481618 , -0.17013389,  1.3932179 , -1.2731404 ,\n",
       "          0.14196219],\n",
       "        [-1.3073918 ,  0.33516636,  0.5894352 , -0.90030533,  0.88234293,\n",
       "          1.3543645 ]], dtype=float32)>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_1 = Dense(in_features = 3, out_features = 6)\n",
    "dense_1.trainable_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 可以看到，初始化後的 dense_1，有兩個 trainable variable，而且起始 weight 也列出來給你看了 \n",
    "* 可以用用看這個 dense 層的功能：輸入 1 x in_features 的 tensor，輸出 1 x out_features 的 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
       "array([[0.       , 0.       , 1.0573964, 1.2296758, 1.6131986, 6.553627 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([[1.0, 2.1, 3.2]])\n",
    "out = dense_1(x)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 可以看到， output 是一個 1x6 的 tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自訂 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 那如果我們想自定一個 model (從 input 一個 tensor，到 output 一個結果出來。重點在定義中間的 forward propagation 過程)，就可以沿用剛剛的定義方式。\n",
    "* 假設，我想做一個 2 個 dense 的 NN (都是linear)，那我可以這樣定義："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "QQ7qQf-DFw74",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model results: tf.Tensor([[4.243844 0.      ]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class SequentialModule(tf.Module):\n",
    "  def __init__(self, name=None):\n",
    "    super().__init__(name=name)\n",
    "\n",
    "    # 在這邊定義好，我等等會用到的 layer 有哪些\n",
    "    self.dense_1 = Dense(in_features=3, out_features=3)\n",
    "    self.dense_2 = Dense(in_features=3, out_features=2)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # forward logic\n",
    "    x = self.dense_1(x)\n",
    "    return self.dense_2(x)\n",
    "\n",
    "# You have made a model!\n",
    "my_model = SequentialModule(name=\"the_model\")\n",
    "\n",
    "# Call it, with random results\n",
    "print(\"Model results:\", my_model(tf.constant([[2.0, 2.0, 2.0]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1oUzasJHHXf"
   },
   "source": [
    "`tf.Module` instances will automatically collect, recursively, any `tf.Variable` or `tf.Module` instances assigned to it. This allows you to manage collections of `tf.Module`s with a single model instance, and save and load whole models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JLFA5_PEGb6C",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submodules: (<__main__.Dense object at 0x14bae3df0>, <__main__.Dense object at 0x10d486550>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Submodules:\", my_model.submodules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "6lzoB8pcRN12",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'b:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)> \n",
      "\n",
      "<tf.Variable 'w:0' shape=(3, 3) dtype=float32, numpy=\n",
      "array([[-2.247787 ,  1.3312311,  1.2570276],\n",
      "       [-1.1049702,  1.2348688,  1.1046218],\n",
      "       [-1.5440533, -0.994251 ,  1.4764204]], dtype=float32)> \n",
      "\n",
      "<tf.Variable 'b:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)> \n",
      "\n",
      "<tf.Variable 'w:0' shape=(3, 2) dtype=float32, numpy=\n",
      "array([[ 0.64399236, -0.01036486],\n",
      "       [ 0.13026635, -0.6883245 ],\n",
      "       [-0.00606108,  2.0914354 ]], dtype=float32)> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for var in my_model.variables:\n",
    "  print(var, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoaxL3zzm0vK"
   },
   "source": [
    "### Waiting to create variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 剛剛可以看到，我們定義的 dense ，要定義 input tensor 的 shape (i.e. in_features 是多少\n",
    "* 這有點麻煩，而且和 keras 內建的 `tf.keras.layers.Dense` 不同\n",
    "* keras 的 Dense 只要知道 out_features 就好， in_features 他會直接去讀你丟給他的 input tensor 來決定\n",
    "* 所以，我們稍微改一下原本的 code，就可以做到這件事："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "XsGCLFXlnPum",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FlexibleDenseModule(tf.Module):\n",
    "  # Note: No need for `in_features`\n",
    "  def __init__(self, out_features, name=None):\n",
    "    super().__init__(name=name)\n",
    "    self.out_features = out_features\n",
    "    # 加入這行\n",
    "    self.is_built = False # 起始狀態時，是 False\n",
    "    \n",
    "  def __call__(self, x):\n",
    "    # 第一次 call 時 (self.is_built 為 False 時)\n",
    "    if not self.is_built:\n",
    "      # 在這裡才定義 w 和 b\n",
    "      self.w = tf.Variable(\n",
    "        tf.random.normal([x.shape[-1], self.out_features]), name='w')\n",
    "      self.b = tf.Variable(tf.zeros([self.out_features]), name='b')\n",
    "      self.is_built = True # 並修改狀態\n",
    "\n",
    "    y = tf.matmul(x, self.w) + self.b\n",
    "    return tf.nn.relu(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "8bjOWax9LOkP",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model results: tf.Tensor([[0. 0.]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Used in a module\n",
    "class MySequentialModule(tf.Module):\n",
    "  def __init__(self, name=None):\n",
    "    super().__init__(name=name)\n",
    "\n",
    "    self.dense_1 = FlexibleDenseModule(out_features=3)\n",
    "    self.dense_2 = FlexibleDenseModule(out_features=2)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    x = self.dense_1(x)\n",
    "    return self.dense_2(x)\n",
    "\n",
    "my_model = MySequentialModule(name=\"the_model\")\n",
    "print(\"Model results:\", my_model(tf.constant([[2.0, 2.0, 2.0]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOLVVBT8J_dl"
   },
   "source": [
    "### Save & load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 剛剛不管是自己定義的 layer，或是 model，因為都繼承自 `tf.Module`，所以都可以追蹤到所有的 trainable variable 的 weights\n",
    "* 那在訓練過程中，我們當然就可以把這些 weights 給存下來，之後就可以從這個 weight 繼續 train 下去  \n",
    "* 存檔的方式有兩種：\n",
    "  * checkpoint: 這就是只有存 weight，沒有存 module/layer structure。所以之後讀取時，要先建立一個一樣 module/layer structure 的 object 後，再把存好的 weight 塞回去\n",
    "  * SaveModel: 這是把 module/layer structure 以及 對應的 weight，全都存下來。之後只要 load 這個 model 就好，大師兄就全都回來了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### checkpoints (save & load weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 作法如下：\n",
    "  * 先用 `checkpoint = tf.train.Checkpoint(model = my_model_obj)`，來建立一個 checkpoint 物件\n",
    "  * 再用 `checkpoint.write('checkpoint_path')`，來把 checkpoint 檔存出來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "pHXKRDk7OLHA",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.Checkpoint at 0x141a2b490>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 先建立一個 checkpoint 物件，他追蹤的是我剛剛 train 到一半的 my_model\n",
    "checkpoint = tf.train.Checkpoint(model=my_model)\n",
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my_checkpoint'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 再把結果寫出來\n",
    "chkp_path = \"my_checkpoint\"\n",
    "checkpoint.write(chkp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 看一下資料夾，寫出兩個檔案：  \n",
    "  * data 本身 (i.e. my_checkpoint.data-00000-of-00001)。裡面包含 variable values and their attribute lookup paths\n",
    "  * index file for metadata (i.e. my_checkpoint.index)。功能是 keeps track of what is actually saved and the numbering of checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_checkpoint.data-00000-of-00001 my_checkpoint.index\n"
     ]
    }
   ],
   "source": [
    "!ls my_checkpoint*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CowCuBTvXgUu"
   },
   "source": [
    "You can look inside a checkpoint to be sure the whole collection of variables is saved, sorted by the Python object that contains them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "o2QAdfpvS8tB",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_CHECKPOINTABLE_OBJECT_GRAPH', []),\n",
       " ('model/dense_1/b/.ATTRIBUTES/VARIABLE_VALUE', [3]),\n",
       " ('model/dense_1/w/.ATTRIBUTES/VARIABLE_VALUE', [3, 3]),\n",
       " ('model/dense_2/b/.ATTRIBUTES/VARIABLE_VALUE', [2]),\n",
       " ('model/dense_2/w/.ATTRIBUTES/VARIABLE_VALUE', [3, 2])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.list_variables(chkp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4eGaNiQWcK4j"
   },
   "source": [
    "During distributed (multi-machine) training they can be sharded,  which is why they are numbered (e.g., '00000-of-00001').  In this case, though, there is only have one shard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 如果之後要把 weight 給 load 進來，那做法是：\n",
    "  * 先建立一個和之前一樣架構的 model/layer. \n",
    "  * 建立 checkpoint 物件，去追蹤這個新建立好的 model/layer\n",
    "  * 用這個新的 checkpoint 物件的 `restore('')` method，把剛剛的 weight 給 load 回來："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "UV8rdDzcwVVg",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = MySequentialModule()\n",
    "new_checkpoint = tf.train.Checkpoint(model=new_model)\n",
    "new_checkpoint.restore(\"my_checkpoint\")\n",
    "\n",
    "# Should be the same result as above\n",
    "new_model(tf.constant([[2.0, 2.0, 2.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQu3TVZecmL7"
   },
   "source": [
    "#### `SavedModel`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 另一種儲存的方式，是直接把 model 的 structure 和 weight 全都存起來  \n",
    "* 作法是：\n",
    "  * 用 `tf.saved_model.save(my_model_obj, \"a_folder_path_to_save_model\")`. \n",
    "  * 用 `new_model = tf.saved_model.load(\"a_folder_path_to_save_model\")` 來讀檔\n",
    "* 看一下範例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Awv_Tw__WK7a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: the_saved_model/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(my_model, \"the_saved_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 可以看到，檔案被存到 `the_saved_model` 這個資料夾中。\n",
    "* 看一下這個資料夾裡有什麼東西：  \n",
    "  * `assets` 資料夾： 空的\n",
    "  * `variables` 資料夾\n",
    "    * variables.data-00000-of-00001: 這就是剛剛 checkpoint 的 data 檔\n",
    "    * variables.index: 這就是剛剛 checkpoint 的 index 檔\n",
    "  * `saved_model.pb` 檔案： a [protocol buffer](https://developers.google.com/protocol-buffers) describing the functional `tf.Graph`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T01:29:46.484880Z",
     "iopub.status.busy": "2021-10-26T01:29:46.469287Z",
     "iopub.status.idle": "2021-10-26T01:29:46.611657Z",
     "shell.execute_reply": "2021-10-26T01:29:46.611235Z"
    },
    "id": "SXv3mEKsefGj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 24\n",
      "drwxr-sr-x 2 kbuilder kokoro  4096 Oct 26 01:29 assets\n",
      "-rw-rw-r-- 1 kbuilder kokoro 14702 Oct 26 01:29 saved_model.pb\n",
      "drwxr-sr-x 2 kbuilder kokoro  4096 Oct 26 01:29 variables\n"
     ]
    }
   ],
   "source": [
    "# Inspect the SavedModel in the directory\n",
    "!ls -l the_saved_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xBqPop7ZesBU"
   },
   "source": [
    "* 讀檔時，這樣讀："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "zRFcA5wIefv4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_model = tf.saved_model.load(\"the_saved_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9EF3mT7i3qN"
   },
   "source": [
    "`new_model`, created from loading a saved model, is an internal TensorFlow user object without any of the class knowledge. It is not of type `SequentialModule`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "EC_eQj7yi54G",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(new_model, SequentialModule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-OrOX1zxiyhR"
   },
   "source": [
    "This new model works on the already-defined input signatures. You can't add more signatures to a model restored like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "_23BYYBWfKnc",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0. 0.]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[0. 0.]\n",
      "  [0. 0.]]], shape=(1, 2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(my_model([[2.0, 2.0, 2.0]]))\n",
    "print(my_model([[[2.0, 2.0, 2.0], [2.0, 2.0, 2.0]]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSFhoMtTjSR6"
   },
   "source": [
    "Thus, using `SavedModel`, you are able to save TensorFlow weights and graphs using `tf.Module`, and then load them again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rb9IdN7hlUZK"
   },
   "source": [
    "## Keras models and layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 剛剛教了用 `tf.Module` 來建立 layer 和 model  \n",
    "* 這邊開始，要來看 Keras 是怎麼用 `tf.Module` 的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uigsVGPreE-D"
   },
   "source": [
    "### Keras layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `tf.keras.layers.Layer` 是 Keras 的 layer 的 base class，他繼承自 `tf.Module`. \n",
    "* 所以，我們剛剛是在 `tf.Module` 的基礎下，建立自己的 layer。現在，可以在 `tf.keras.layers.Layer` 的基礎下，建立 layer。  \n",
    "* 那這樣的好處是，就可以繼承更多 keras 的 layer 所擁有的 attribute, methods，使得，未來用 keras 的 fit, compile 等功能時，他去吃你定義的 dense，都能取得他預期要拿到的東西  \n",
    "* 換句話說，如果你後續想用 keras 的 compile, fit 等功能，那你的 layer，必須繼承 keras layer 的 class，而不是 `tf.Module`. \n",
    "* 我們來看已下範例，寫法基本上和剛剛沒差別，只有 `__call__` 要改成 `call`，因為 keras 自己有定義 call method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "88YOGquhnQRd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyDense(tf.keras.layers.Layer):\n",
    "  # Adding **kwargs to support base Keras layer arguments\n",
    "  def __init__(self, in_features, out_features, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "\n",
    "    # This will soon move to the build step; see below\n",
    "    self.w = tf.Variable(\n",
    "      tf.random.normal([in_features, out_features]), name='w')\n",
    "    self.b = tf.Variable(tf.zeros([out_features]), name='b')\n",
    "  def call(self, x):\n",
    "    y = tf.matmul(x, self.w) + self.b\n",
    "    return tf.nn.relu(y)\n",
    "\n",
    "simple_layer = MyDense(name=\"simple\", in_features=3, out_features=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYGmAsPrws--"
   },
   "source": [
    "Keras layers have their own `__call__` that does some bookkeeping described in the next section and then calls `call()`. You should notice no change in functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "nIqE8wOznYKG",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[0.      , 0.      , 6.451925]], dtype=float32)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_layer([[2.0, 2.0, 2.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmN5vb1K18U1"
   },
   "source": [
    "### The `build` step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 再來，是 keras 的 layer 寫法中，很重要的 `build` step. \n",
    "* 還記得前面用 `tf.Module` 來寫 layer 時，為了不要每次都定義 input_feature，所以會先寫一個 `self.is_built = False` 來說明，目前還沒被 build。然後當 input tensor 進來後，才定義 w 和 b 的 shape，並把 `self.is_built` 改為 true，表示已經 build 完\n",
    "* 那，keras 這邊，就直接定義一個 method 叫 build，就是直接用來取 input_feature 用的\n",
    "* `build` is called exactly once, and it is called with the shape of the input. It's usually used to create variables (weights).\n",
    "* 我們來改寫一下剛剛的 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "4YTfrlgdsURp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FlexibleDense(tf.keras.layers.Layer):\n",
    "  # Note the added `**kwargs`, as Keras supports many arguments\n",
    "  def __init__(self, out_features, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.out_features = out_features\n",
    "\n",
    "  def build(self, input_shape):  # Create the state of the layer (weights)\n",
    "    self.w = tf.Variable(\n",
    "      tf.random.normal([input_shape[-1], self.out_features]), name='w')\n",
    "    self.b = tf.Variable(tf.zeros([self.out_features]), name='b')\n",
    "\n",
    "  def call(self, inputs):  # Defines the computation from inputs to outputs\n",
    "    return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 上面的寫法，有幾個重點要注意：  \n",
    "  * `__init__` 裡面，多了 `**kwargs**`，因為，繼承自 `tf.keras.layers.Layer` 時，他已經 support 很多其他的 input arguments 了. \n",
    "  * `build(self, input_shape)` 這個 method中，input_shape 是 keras 會自動幫你讀出 input tensor 的 shape。所以下面就會用 input_shape[-1] 來當作 input_feature 數。\n",
    "  * `build` 這個 method，你不會真的拿來用，他就是一個輔助 method，當你第一次把 input tensor 丟進去時，他會自己啟用\n",
    "* 接著，我們實例化這個 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the instance of the layer\n",
    "flexible_dense = FlexibleDense(out_features=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Koc_uSqt2PRh"
   },
   "source": [
    "* 目前為止，我們還沒丟 input tensor 進去，所以 build 方法還沒被啟用，這時候，我們的 layer，還沒追蹤到任何 variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "DgyTyUD32Ln4",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flexible_dense.variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 一旦我丟一個 input tensor 進去後，就會啟用 build 方法，weight 就被 initialize 了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "IkLyEx7uAoTK",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model results: tf.Tensor(\n",
      "[[-0.81454504 -4.95683     1.3514652 ]\n",
      " [-1.2218176  -7.435245    2.027198  ]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Call it, with predictably random results\n",
    "print(\"Model results:\", flexible_dense(tf.constant([[2.0, 2.0, 2.0], [3.0, 3.0, 3.0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Swofpkrd2YDd",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'flexible_dense_1/w:0' shape=(3, 3) dtype=float32, numpy=\n",
       " array([[-0.74692434, -1.0260396 ,  0.7583028 ],\n",
       "        [ 0.05714832, -0.89261234,  0.0625408 ],\n",
       "        [ 0.2825035 , -0.5597631 , -0.14511095]], dtype=float32)>,\n",
       " <tf.Variable 'flexible_dense_1/b:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flexible_dense.variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PuNUnf0OIpF"
   },
   "source": [
    "* 由於 `build` method 只會被 call 一次，所以如果之後 input 的 tensor，shape 和 起始話的時候不同，那就會報 error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "caYWDrHSAy_j",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed: Matrix size-incompatible: In[0]: [1,4], In[1]: [3,3] [Op:MatMul]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  print(\"Model results:\", flexible_dense(tf.constant([[2.0, 2.0, 2.0, 2.0]])))\n",
    "except tf.errors.InvalidArgumentError as e:\n",
    "  print(\"Failed:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2kds2IHw2KD"
   },
   "source": [
    "### Keras models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 前面在定義 layer 和 model 時，都是繼承自 `tf.Module` 這個 class  \n",
    "* 但當我們要定義 custom keras layer 時，我們會繼承自 `tf.keras.layers.Layer`(此 class 繼承自 `tf.Module`)，這樣就能保有很多 keras layer 的好的特性\n",
    "* 同樣的，當我們要定義 custom keras model 時，我們也會繼承一個 keras 的 class，就是 `tf.keras.Model` (此 class 繼承自 `tf.keras.layers.Layer`)  \n",
    "* 這樣做的好處是，我們定義好的 Model，可以輕易的被 used, nested, and saved in the same way as Keras layers.\n",
    "* 而且，還會有許多 extra functionality that makes them easy to train, evaluate, load, save, and even train on multiple machines.\n",
    "* 來寫吧："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Hqjo1DiyrHrn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MySequentialModel(tf.keras.Model):\n",
    "  def __init__(self, name=None, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "\n",
    "    self.dense_1 = FlexibleDense(out_features=3)\n",
    "    self.dense_2 = FlexibleDense(out_features=2)\n",
    "  def call(self, x):\n",
    "    x = self.dense_1(x)\n",
    "    return self.dense_2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 可以看到，比剛剛寫 layer 簡單，因為不用寫 `build` method。這部分在定義 custom layer 時已經做完了。只要單純的 `__init__` 和 `call` 就好\n",
    "* 實例化這個 model，一樣的，一開始找不到 weight，因位還沒有 tensor 被餵進來，還不知道 input shape，就不會 initialze weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You have made a Keras model!\n",
    "my_sequential_model = MySequentialModel(name=\"the_model\")\n",
    "my_sequential_model.variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 丟個 input tensor 進去，就可以看到結果，以及 variables 了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model results: tf.Tensor([[-2.5422215  2.2206373]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Call it on a tensor, with random results\n",
    "print(\"Model results:\", my_sequential_model(tf.constant([[2.0, 2.0, 2.0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "hdLQFNdMsOz1",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'my_sequential_model/flexible_dense_2/w:0' shape=(3, 3) dtype=float32, numpy=\n",
       " array([[-0.31155884,  0.35502377,  0.13043131],\n",
       "        [ 0.2252853 , -0.0766846 , -1.9106293 ],\n",
       "        [ 0.2285221 ,  1.0349969 ,  0.52284014]], dtype=float32)>,\n",
       " <tf.Variable 'my_sequential_model/flexible_dense_2/b:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'my_sequential_model/flexible_dense_3/w:0' shape=(3, 2) dtype=float32, numpy=\n",
       " array([[-0.8085942 ,  0.611606  ],\n",
       "        [-1.0512786 ,  0.78703386],\n",
       "        [-0.17862263,  0.00820879]], dtype=float32)>,\n",
       " <tf.Variable 'my_sequential_model/flexible_dense_3/b:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sequential_model.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "JjVAMrAJsQ7G",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<__main__.FlexibleDense at 0x142e349a0>,\n",
       " <__main__.FlexibleDense at 0x13692cfa0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sequential_model.submodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_sequential_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flexible_dense_2 (FlexibleDe multiple                  12        \n",
      "_________________________________________________________________\n",
      "flexible_dense_3 (FlexibleDe multiple                  8         \n",
      "=================================================================\n",
      "Total params: 20\n",
      "Trainable params: 20\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_sequential_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 另外一種建立 model 的方式，是直接用 functional API，這不僅可以幫我們減少一些時間，也可以獲得一些額外的好處 (e.g. model.summary()時，可發現 output shape 都跑出來了)  \n",
    "* functional API 和剛剛 subclass 的寫法，最大差別在，你要先定義 input 的 shape (by `tf.keras.Input(shape = [3,])`)\n",
    "* The `input_shape` argument in this case does not have to be completely specified; you can leave some dimensions as `None`.\n",
    "* Note: You do not need to specify `input_shape` or an `InputLayer` in a subclassed model; these arguments and layers will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "jJiZZiJ0fyqQ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 3)]               0         \n",
      "_________________________________________________________________\n",
      "flexible_dense_4 (FlexibleDe (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "flexible_dense_5 (FlexibleDe (None, 2)                 8         \n",
      "=================================================================\n",
      "Total params: 20\n",
      "Trainable params: 20\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.Input(shape=[3,])\n",
    "x = FlexibleDense(3)(inputs)\n",
    "x = FlexibleDense(2)(x)\n",
    "\n",
    "my_functional_model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "my_functional_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "kg-xAZw5gaG6",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.3817817, -3.4696531]], dtype=float32)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_functional_model(tf.constant([[2.0, 2.0, 2.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qI9aXLnaHEFF"
   },
   "source": [
    "## Saving Keras models\n",
    "\n",
    "Keras models can be checkpointed, and that will look the same as `tf.Module`.\n",
    "\n",
    "Keras models can also be saved with `tf.saved_model.save()`, as they are modules.  However, Keras models have convenience methods and other functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "SAz-KVZlzAJu",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/hanklee/.pyenv/versions/3.8.0/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /Users/hanklee/.pyenv/versions/3.8.0/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: exname_of_file/assets\n"
     ]
    }
   ],
   "source": [
    "my_sequential_model.save(\"exname_of_file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2urAeR-omns"
   },
   "source": [
    "Just as easily, they can be loaded back in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "Wj5DW-LCopry",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "reconstructed_model = tf.keras.models.load_model(\"exname_of_file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EA7P_MNvpviZ"
   },
   "source": [
    "Keras `SavedModels` also save metric, loss, and optimizer states.\n",
    "\n",
    "This reconstructed model can be used and will produce the same result when called on the same data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "P_wGfQo5pe6T",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-2.5422215,  2.2206373]], dtype=float32)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructed_model(tf.constant([[2.0, 2.0, 2.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKyjlkceqjwD"
   },
   "source": [
    "There is more to know about saving and serialization of Keras models, including providing configuration methods for custom layers for feature support. Check out the [guide to saving and serialization](https://www.tensorflow.org/guide/keras/save_and_serialize)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcdMMPYv7Krz"
   },
   "source": [
    "# What's next\n",
    "\n",
    "If you want to know more details about Keras, you can follow the existing Keras guides [here](./keras/).\n",
    "\n",
    "Another example of a high-level API built on `tf.module` is Sonnet from DeepMind, which is covered on [their site](https://github.com/deepmind/sonnet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 待完成內容："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnporXiudF1I"
   },
   "source": [
    "Keras layers have a lot more extra features including:\n",
    "\n",
    "* Optional losses\n",
    "* Support for metrics\n",
    "* Built-in support for an optional `training` argument to differentiate between training and inference use\n",
    "* `get_config` and `from_config` methods that allow you to accurately store configurations to allow model cloning in Python\n",
    "\n",
    "Read about them in the [full guide](./keras/custom_layers_and_models.ipynb) to custom layers and models."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ISubpr_SSsiM"
   ],
   "name": "intro_to_modules.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
